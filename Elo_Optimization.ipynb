{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm,tqdm_notebook \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内存优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.4 s, sys: 11.6 s, total: 60 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "new_transactions = pd.read_csv('../../data/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "historical_transactions = pd.read_csv('../../data/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "for col in ['authorized_flag', 'category_1']:\n",
    "    historical_transactions[col] = historical_transactions[col].map({'Y':1, 'N':0})\n",
    "    new_transactions[col]        = new_transactions[col].map({'Y':1, 'N':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 5.78 s, total: 16.6 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 加载训练集，测试集，基本处理\n",
    "train = pd.read_csv('../../data/train.csv')\n",
    "test = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "target = train['target']\n",
    "for df in [train, test]:    \n",
    "    df['year']  = df['first_active_month'].fillna('0-0').apply(lambda x:int(str(x).split('-')[0]))\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['elapsed_time'] = (datetime.date(2018,3, 1) - df['first_active_month'].dt.date).dt.days\n",
    "    \n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['dayofyear'] = df['first_active_month'].dt.dayofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    \n",
    "## 交易表合并train test\n",
    "train_test = pd.concat([train[['card_id','first_active_month']], test[['card_id','first_active_month']] ], axis=0, ignore_index=True)\n",
    "historical_transactions   = historical_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')\n",
    "new_transactions = new_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 3192.83 Mb (64.1% reduction)\n",
      "Mem. usage decreased to 211.55 Mb (64.7% reduction)\n",
      "CPU times: user 4min 42s, sys: 32.8 s, total: 5min 15s\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def month_trans(x): \n",
    "    return x // 30\n",
    "\n",
    "def week_trans(x): \n",
    "    return x // 7\n",
    "\n",
    "## 交易表预处理\n",
    "def get_expand_common(df_):\n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['category_2'].fillna(1.0,inplace=True)\n",
    "    df['category_3'].fillna('A',inplace=True)\n",
    "    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    df['installments'].replace(999, np.nan,inplace=True)\n",
    "    \n",
    "    df['purchase_amount'] = np.round(df['purchase_amount'] / 0.00150265118 + 497.06,8)\n",
    "    df['purchase_amount'] = df.purchase_amount.apply(lambda x: np.round(x))\n",
    "    \n",
    "    df['purchase_date']          =  pd.to_datetime(df['purchase_date']) \n",
    "    df['first_active_month']     =  pd.to_datetime(df['first_active_month']) \n",
    "    df['purchase_hour']          =  df['purchase_date'].dt.hour\n",
    "    df['year']                   = df['purchase_date'].dt.year\n",
    "    df['month']                  =  df['purchase_date'].dt.month\n",
    "    df['day']                    = df['purchase_date'].dt.day\n",
    "    df['hour']                   = df['purchase_date'].dt.hour\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['dayofweek']              =  df['purchase_date'].dt.dayofweek\n",
    "    df['weekend']                =  (df.purchase_date.dt.weekday >=5).astype(int) \n",
    "    df                           =  df.sort_values(['card_id','purchase_date']) \n",
    "    df['purchase_date_floorday'] =  df['purchase_date'].dt.floor('d')  #删除小于day的时间\n",
    "    \n",
    "    # 距离激活时间的相对时间,0, 1,2,3,...,max-act\n",
    "    df['purchase_day_since_active_day']   = df['purchase_date_floorday'] - df['first_active_month']  #ht_card_id_gp['purchase_date_floorday'].transform('min')\n",
    "    df['purchase_day_since_active_day']   = df['purchase_day_since_active_day'].dt.days  #.astype('timedelta64[D]') \n",
    "    df['purchase_month_since_active_day'] = df['purchase_day_since_active_day'].agg(month_trans).values\n",
    "    df['purchase_week_since_active_day']  = df['purchase_day_since_active_day'].agg(week_trans).values\n",
    "    \n",
    "    # 距离最后一天时间的相对时间,0,1,2,3,...,max-act\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    df['purchase_day_since_reference_day']   =  ht_card_id_gp['purchase_date_floorday'].transform('max') - df['purchase_date_floorday']\n",
    "    df['purchase_day_since_reference_day']   =  df['purchase_day_since_reference_day'].dt.days\n",
    "    # 一个粗粒度的特征(距离最近购买过去了几周，几月)\n",
    "    df['purchase_week_since_reference_day']  = df['purchase_day_since_reference_day'].agg(week_trans).values\n",
    "    df['purchase_month_since_reference_day'] = df['purchase_day_since_reference_day'].agg(month_trans).values\n",
    "    \n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].shift()\n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].values - df['purchase_day_diff'].values\n",
    "    df['purchase_day_diff']   =  df['purchase_day_diff'].dt.days\n",
    "    df['purchase_week_diff']  =  df['purchase_day_diff'].agg(week_trans).values\n",
    "    df['purchase_month_diff'] =  df['purchase_day_diff'].agg(month_trans).values \n",
    "    \n",
    "    df['purchase_amount_ddgd_98']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.98**x).values\n",
    "    df['purchase_amount_ddgd_99']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.99**x).values    \n",
    "    df['purchase_amount_wdgd_96']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.96**x).values \n",
    "    df['purchase_amount_wdgd_97']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.97**x).values \n",
    "    df['purchase_amount_mdgd_90']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.9**x).values\n",
    "    df['purchase_amount_mdgd_80']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.8**x).values \n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "historical_transactions = get_expand_common(historical_transactions)\n",
    "new_transactions        = get_expand_common(new_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征优化部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate statistics features...\n",
      "generate statistics features...\n",
      "generate statistics features...\n",
      "CPU times: user 2min 42s, sys: 16.2 s, total: 2min 58s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 构造基本统计特征\n",
    "def aggregate_transactions(df_, prefix): \n",
    "    \n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "    df['duration'] = df['purchase_amount'] * df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount'] / df['month_diff']\n",
    "    \n",
    "    df.loc[:, 'purchase_date'] = pd.DatetimeIndex(df['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1':      ['mean'],\n",
    "        'category_2':      ['mean'],\n",
    "        'category_3':      ['mean'],\n",
    "        'installments':    ['mean', 'max', 'min', 'std'],\n",
    "        'month_lag':       ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'month':           ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'hour':            ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'weekofyear':      ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'dayofweek':       ['nunique', 'mean'],\n",
    "        'weekend':         ['mean'],\n",
    "        'year':            ['nunique'],\n",
    "        'card_id':         ['size','count'],\n",
    "        'purchase_date':   ['max', 'min'],\n",
    "        ###\n",
    "        'price':             ['mean','max','min','std'],\n",
    "        'duration':          ['mean','min','max','std','skew'],\n",
    "        'amount_month_ratio':['mean','min','max','std','skew'],\n",
    "        } \n",
    "    \n",
    "    for col in ['category_2','category_3']:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        agg_func[col+'_mean'] = ['mean']\n",
    "    \n",
    "    agg_df = df.groupby(['card_id']).agg(agg_func)\n",
    "    agg_df.columns = [prefix + '_'.join(col).strip() for col in agg_df.columns.values]\n",
    "    agg_df.reset_index(drop=False, inplace=True)\n",
    "  \n",
    "    return agg_df\n",
    "print('generate statistics features...')\n",
    "auth_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==1], prefix='auth_')\n",
    "print('generate statistics features...')\n",
    "hist_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==0], prefix='hist_')\n",
    "print('generate statistics features...')\n",
    "new_base_stat  = aggregate_transactions(new_transactions, prefix='new_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      "reference_day 不存在！！！\n",
      "first_day 不存在！！！\n",
      "last_day 不存在！！！\n",
      "activation_day 不存在！！！\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddcf02088db4d96bc7df2e25d49972f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770521a16293420f98b1cbabc8363985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c10f405d7e54097886c6f5e0f420bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 112.08 Mb (73.1% reduction)\n",
      "hist...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      "card_id(month_lag, min to reference day):min\n",
      "last_day 不存在！！！\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4001a1f72f49a79a7fdd7e5d805abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da7d99ac5a74924ac55398d761dfb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7f9b4d07734dd79261e257f032b2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 121.08 Mb (72.6% reduction)\n",
      "new...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      " Eight time features, \n",
      "card_id(month_lag, min to reference day):min\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757f793f5a8d40458e9b60178fe603f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c11ce91be0b4f2cbed0bcdac8ae6bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ace0e38a214d82abe0574c7b7ce414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Part2， data with time less than activation day ******************************\n",
      "card_id(purchase_amount): sum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2b7bba092a49899ace1a56e5c9915c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 92.65 Mb (73.0% reduction)\n",
      "CPU times: user 6min 42s, sys: 58.5 s, total: 7min 40s\n",
      "Wall time: 7min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_quantile(x, percentiles = [0.1, 0.25, 0.75, 0.9]):\n",
    "    x_len = len(x)\n",
    "    x = np.sort(x)\n",
    "    sts_feas = []  \n",
    "    for per_ in percentiles:\n",
    "        if per_ == 1:\n",
    "            sts_feas.append(x[x_len - 1]) \n",
    "        else:\n",
    "            sts_feas.append(x[int(x_len * per_)]) \n",
    "    return sts_feas \n",
    "\n",
    "def get_cardf_tran(df_, month = 3, prefix = '_'):\n",
    "    \n",
    "    df = df_.copy() \n",
    "    if prefix == 'hist_cardf_':\n",
    "        df['month_to_now']  =  (datetime.date(2018, month, 1) - df['purchase_date_floorday'].dt.date).dt.days\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id') \n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    if  prefix == 'hist_cardf_':\n",
    "        cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values\n",
    "        cardid_features['card_id_isau_sum'] = ht_card_id_gp['authorized_flag'].sum().values \n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "    cardid_features['month_diff_median'] = ht_card_id_gp['month_diff'].median().values\n",
    "    \n",
    "    if prefix == 'hist_cardf_':\n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "       \n",
    "        # first to activation day\n",
    "        cardid_features['first_to_activation_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to reference day \n",
    "        cardid_features['activation_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['activation_day']).dt.days\n",
    "        # first to last day \n",
    "        cardid_features['first_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['first_day']).dt.days\n",
    "        # reference day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_min'] = ht_card_id_gp['month_lag'].agg('min').values   \n",
    "        # is_purchase_before_activation,first_to_reference_day_divide_activation_to_reference_day\n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['first_to_activation_day'] < 0 \n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['is_purchase_before_activation'].astype(int)\n",
    "        cardid_features['first_to_reference_day_divide_activation_to_reference_day'] = cardid_features['first_to_reference_day']  / (cardid_features['activation_to_reference_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_reference_day'].values / cardid_features['card_id_cnt'].values\n",
    "   \n",
    "    if prefix == 'new_cardf_':\n",
    "        print(' Eight time features, ') \n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['reference_day'].last().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['last_day']                =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "        # reference to first day\n",
    "        cardid_features['reference_day_to_first_day']  =  (cardid_features['first_day'] - cardid_features['reference_day']).dt.days\n",
    "        # reference to last day\n",
    "        cardid_features['reference_day_to_last_day']  =  (cardid_features['last_day'] - cardid_features['reference_day']).dt.days  \n",
    "        # first to last day \n",
    "        cardid_features['first_to_last_day']  =  (cardid_features['last_day'] - cardid_features['first_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_first_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_last_day']  =  (cardid_features['last_day'] - cardid_features['activation_day']).dt.days\n",
    "        # last day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_max'] = ht_card_id_gp['month_lag'].agg('max').values  \n",
    "        cardid_features['first_to_last_day_divide_reference_to_last_day'] = cardid_features['first_to_last_day']  / (cardid_features['reference_day_to_last_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_last_day'].values / cardid_features['card_id_cnt'].values\n",
    "    \n",
    "    for f in ['reference_day', 'first_day', 'last_day', 'activation_day']:\n",
    "        try:\n",
    "            del cardid_features[f]\n",
    "        except:\n",
    "            print(f, '不存在！！！')\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['category_1','category_2','category_3','state_id','city_id','installments','merchant_id', 'merchant_category_id','subsector_id','month_lag','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col]            =  ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] =  cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    print('card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)') \n",
    "    for col in tqdm_notebook(['installments','purchase_amount','purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median','max','min']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values\n",
    "            \n",
    "            cardid_features['card_id_' +col+ '_range'] =  cardid_features['card_id_' +col+ '_max'].values - cardid_features['card_id_' +col+ '_min'].values\n",
    "            percentiles = ht_card_id_gp[col].apply(lambda x:get_quantile(x,percentiles = [0.025, 0.25, 0.75, 0.975])) \n",
    "\n",
    "            cardid_features[col + '_2.5_quantile']  = percentiles.map(lambda x:x[0]).values\n",
    "            cardid_features[col + '_25_quantile'] = percentiles.map(lambda x:x[1]).values\n",
    "            cardid_features[col + '_75_quantile'] = percentiles.map(lambda x:x[2]).values\n",
    "            cardid_features[col + '_97.5_quantile'] = percentiles.map(lambda x:x[3]).values\n",
    "            cardid_features['card_id_' +col+ '_range2'] =  cardid_features[col+ '_97.5_quantile'].values - cardid_features[col+ '_2.5_quantile'].values\n",
    "            del cardid_features[col + '_2.5_quantile'],cardid_features[col + '_97.5_quantile']\n",
    "            gc.collect()\n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values          \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大installments\n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id',,\n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "    \n",
    "    if prefix == 'new_cardf_':\n",
    "    ######## 在卡未激活之前就有过消费的记录  ##############   \n",
    "        print('*'*30,'Part2， data with time less than activation day','*'*30)\n",
    "        df_part = df.loc[df.purchase_date < df.first_active_month]\n",
    "\n",
    "        cardid_features_part = pd.DataFrame()\n",
    "        cardid_features_part['card_id'] = df_part['card_id'].unique()   \n",
    "        ht_card_id_part_gp = df_part.groupby('card_id')\n",
    "        cardid_features_part['card_id_part_cnt'] = ht_card_id_part_gp['authorized_flag'].count().values\n",
    "\n",
    "        print('card_id(purchase_amount): sum') \n",
    "        for col in tqdm_notebook(['purchase_amount']): \n",
    "            for opt in ['sum','mean']:\n",
    "                cardid_features_part['card_id_part_' +col+ '_' + opt] = ht_card_id_part_gp[col].agg(opt).values\n",
    "\n",
    "        cardid_features = cardid_features.merge(cardid_features_part, on ='card_id', how='left')\n",
    "        cardid_features['card_id_part_purchase_amount_sum_percent'] = cardid_features['card_id_part_purchase_amount_sum'] / (cardid_features['card_id_purchase_amount_sum'] + 0.01)\n",
    "\n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features\n",
    "print('auth...')\n",
    "authorized_transactions = historical_transactions.loc[historical_transactions['authorized_flag'] == 1]\n",
    "auth_cardf_tran = get_cardf_tran(authorized_transactions, 3, prefix='auth_cardf_')\n",
    "print('hist...')\n",
    "hist_cardf_tran = get_cardf_tran(historical_transactions, 3, prefix='hist_cardf_')\n",
    "print('new...')\n",
    "reference_days = historical_transactions.groupby('card_id')['purchase_date'].last().to_frame('reference_day')\n",
    "reference_days.reset_index(inplace = True)\n",
    "new_transactions = new_transactions.merge(reference_days, on ='card_id', how='left')\n",
    "new_cardf_tran  = get_cardf_tran(new_transactions, 5, prefix='new_cardf_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      " card id : count\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4e063af0cb4a3589ffb5f62d8bd72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336ac6882c5246f39b457100fe6a540f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1487423a6b7744e5a4147052311c55aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 84.45 Mb (74.0% reduction)\n",
      "CPU times: user 1min 2s, sys: 4.31 s, total: 1min 6s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_cardf_tran_last2(df_, month = 3, prefix = 'last2_'): \n",
    "    \n",
    "    df = df_.loc[df_.month_lag >= -2].copy()\n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    print(' card id : count')\n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values \n",
    "    cardid_features['card_id_isau_sum']  = ht_card_id_gp['authorized_flag'].sum().values\n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['state_id','city_id','installments','merchant_id', 'merchant_category_id','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col] = ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] = cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    for col in tqdm_notebook(['purchase_amount','purchase_amount_ddgd_98','purchase_amount_wdgd_96','purchase_amount_mdgd_90','purchase_amount_mdgd_80']): #,'purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values  \n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大\n",
    "    \n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id', \n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "     \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "hist_cardf_tran_last2 = get_cardf_tran_last2(historical_transactions, month = 3, prefix = 'hist_last2_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hist...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036444bc36fa4df69340e5d585527376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaea1fc19a9a41938c3858566ff6419a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e5d6a37a5242f0b2ff24c837bbfbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 70.16 Mb (66.8% reduction)\n",
      "CPU times: user 4min 2s, sys: 40.3 s, total: 4min 42s\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def successive_aggregates(df_, prefix = 'levelAB_'):\n",
    "    df = df_.copy()\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()    \n",
    "     \n",
    "    level12_nunique = [('month_lag','state_id'),('month_lag','city_id'),('month_lag','subsector_id'),('month_lag','merchant_category_id'),('month_lag','merchant_id'),('month_lag','purchase_date_floorday'),\\\n",
    "                       ('subsector_id','merchant_category_id'),('subsector_id','merchant_id'),('subsector_id','purchase_date_floorday'),('subsector_id','month_lag'),\\\n",
    "                       ('merchant_category_id', 'merchant_id'),('merchant_category_id','purchase_date_floorday'),('merchant_category_id','month_lag'),\\\n",
    "                       ('purchase_date_floorday', 'merchant_id'),('purchase_date_floorday','merchant_category_id'),('purchase_date_floorday','subsector_id')]    \n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_nunique):  \n",
    "        \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].nunique().to_frame(col_level2 + '_nunique')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_nunique'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_nunique_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_count = ['month_lag','state_id','city_id','subsector_id','merchant_category_id','merchant_id','purchase_date_floorday']\n",
    "    for col_level in tqdm_notebook(level12_count): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level])['merchant_id'].count().to_frame(col_level + '_count')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level + '_count'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level + '_count_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_meansum = [('month_lag','purchase_amount'),('state_id','purchase_amount'),('city_id','purchase_amount'),('subsector_id','purchase_amount'),\\\n",
    "                       ('merchant_category_id','purchase_amount'),('merchant_id','purchase_amount'),('purchase_date_floorday','purchase_amount')]\n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_meansum): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].sum().to_frame(col_level2 + '_sum')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_sum'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_sum_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "\n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left')           \n",
    "    \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "print('hist...')\n",
    "hist_levelAB = successive_aggregates(historical_transactions, prefix = 'hist_levelAB_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 11)\n",
      "(123623, 10)\n",
      "#_____基础统计特征\n",
      "(201917, 164)\n",
      "(123623, 163)\n",
      "#_____全局cardid特征\n",
      "(201917, 687)\n",
      "(123623, 686)\n",
      "#_____最近两月cardid特征\n",
      "(201917, 821)\n",
      "(123623, 820)\n",
      "#_____补充二阶特征\n",
      "(201917, 911)\n",
      "(123623, 910)\n",
      "CPU times: user 21.8 s, sys: 1.45 s, total: 23.3 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "## 合并到训练集和测试集\n",
    "print('#_____基础统计特征')\n",
    "train = pd.merge(train, auth_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, new_base_stat , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_base_stat , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____全局cardid特征')\n",
    "train = pd.merge(train, auth_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, new_cardf_tran , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_cardf_tran , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____最近两月cardid特征')\n",
    "train = pd.merge(train, hist_cardf_tran_last2, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran_last2, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____补充二阶特征')\n",
    "train = pd.merge(train, hist_levelAB, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_levelAB, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 405 ms, sys: 683 ms, total: 1.09 s\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()\n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    colname = f+'_outliers_mean'\n",
    "    order_label = train.groupby([f])['outliers'].mean()\n",
    "    for df in [train, test]:\n",
    "        df[colname] = df[f].map(order_label)\n",
    "\n",
    "for df in [train, test]:\n",
    "    \n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "    \n",
    "    df['card_id_total'] = df['hist_card_id_size']+df['new_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['hist_card_id_count']+df['new_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['hist_cardf_card_id_purchase_amount_sum']+df['new_cardf_card_id_purchase_amount_sum']\n",
    "    df['purchase_amount_ratio'] = df['new_cardf_card_id_purchase_amount_sum']/df['hist_cardf_card_id_purchase_amount_sum']\n",
    "    df['month_diff_ratio'] = df['new_cardf_month_diff_mean']/df['hist_cardf_month_diff_mean']\n",
    "    df['installments_total'] = df['new_cardf_card_id_installments_sum']+df['auth_cardf_card_id_installments_sum']\n",
    "    df['installments_ratio'] = df['new_cardf_card_id_installments_sum']/df['auth_cardf_card_id_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total']/df['installments_total']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_cardf_card_id_purchase_amount_sum'] / df['new_cardf_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_cardf_card_id_purchase_amount_sum'] / df['hist_cardf_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征基本过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运用杰哥方法...\n",
      "删除前: 938\n",
      "删除后: 770\n",
      "CPU times: user 2min 11s, sys: 3.81 s, total: 2min 15s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "del_cols = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'new_cardf': \n",
    "        del_cols.append(col)\n",
    "del_cols1 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'hist_last2_' in col:\n",
    "        del_cols1.append(col)\n",
    "del_cols2 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'auth_cardf' in col:\n",
    "        del_cols2.append(col)\n",
    "del_cols3 = []\n",
    "for col in train.columns:\n",
    "    if 'merchant_category_id_month_lag_nunique_' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'city_id' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff' in col and 'hist_last2_' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff_std' in col or 'month_diff_gap' in col:\n",
    "        del_cols3.append(col) \n",
    "fea_cols = [col for col in train.columns if train[col].dtypes!='object' and train[col].dtypes != '<M8[ns]' and col!='target' not in col and col!='min_num'\\\n",
    "            and col not in del_cols and col not in del_cols1 and col not in del_cols2 and col!='target1' and col!='card_id_cnt_ht_pivot_supp'  and col not in del_cols3]   \n",
    "print('运用杰哥方法...')\n",
    "print('删除前:',train.shape[1])\n",
    "print('删除后:',len(fea_cols))\n",
    "\n",
    "train = train[fea_cols+['target']]\n",
    "fea_cols.remove('outliers')\n",
    "test = test[fea_cols]\n",
    "\n",
    "train.to_csv('../../data/all_train_features.csv',index=False)\n",
    "test.to_csv('../../data/all_test_features.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (201917, 771)\n",
      "ntrain: (199710, 771)\n",
      "CPU times: user 19.8 s, sys: 4.92 s, total: 24.8 s\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load all features\n",
    "train = pd.read_csv('../../data/all_train_features.csv')\n",
    "test  = pd.read_csv('../../data/all_test_features.csv')\n",
    "\n",
    "# ## load sparse\n",
    "# train_tags = sparse.load_npz('train_tags.npz')\n",
    "# test_tags  = sparse.load_npz('test_tags.npz')\n",
    "\n",
    "## 获取非异常值的index\n",
    "normal_index = train[train['outliers']==0].index.tolist()\n",
    "## without outliers\n",
    "ntrain = train[train['outliers'] == 0]\n",
    "\n",
    "target        = train['target'].values\n",
    "ntarget       = ntrain['target'].values\n",
    "target_binary = train['outliers'].values\n",
    "###\n",
    "y_train        = target\n",
    "y_ntrain       = ntarget\n",
    "y_train_binary = target_binary\n",
    "\n",
    "print('train:',train.shape)\n",
    "print('ntrain:',ntrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds, model_type='lgb', eval_type='regression'):\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    for fold_n, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            trn_data = lgb.Dataset(X[trn_idx], y[trn_idx])\n",
    "            val_data = lgb.Dataset(X[val_idx], y[val_idx])\n",
    "            clf = lgb.train(params, trn_data, num_boost_round=20000, \n",
    "                            valid_sets=[trn_data, val_data], \n",
    "                            verbose_eval=100, early_stopping_rounds=300)\n",
    "            oof[val_idx] = clf.predict(X[val_idx], num_iteration=clf.best_iteration)\n",
    "            predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        \n",
    "        if model_type == 'xgb':\n",
    "            trn_data = xgb.DMatrix(X[trn_idx], y[trn_idx])\n",
    "            val_data = xgb.DMatrix(X[val_idx], y[val_idx])\n",
    "            watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "            clf = xgb.train(dtrain=trn_data, num_boost_round=20000, \n",
    "                            evals=watchlist, early_stopping_rounds=200, \n",
    "                            verbose_eval=100, params=params)\n",
    "            oof[val_idx] = clf.predict(xgb.DMatrix(X[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "            predictions += clf.predict(xgb.DMatrix(X_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "        \n",
    "        if (model_type == 'cat') and (eval_type == 'regression'):\n",
    "            clf = CatBoostRegressor(iterations=20000, eval_metric='RMSE', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict(X[val_idx])\n",
    "            predictions += clf.predict(X_test) / folds.n_splits\n",
    "            \n",
    "        if (model_type == 'cat') and (eval_type == 'binary'):\n",
    "            clf = CatBoostClassifier(iterations=20000, eval_metric='Logloss', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict_proba(X[val_idx])[:,1]\n",
    "            predictions += clf.predict_proba(X_test)[:,1] / folds.n_splits\n",
    "        print(predictions)\n",
    "        if eval_type == 'regression':\n",
    "            scores.append(mean_squared_error(oof[val_idx], y[val_idx])**0.5)\n",
    "        if eval_type == 'binary':\n",
    "            scores.append(log_loss(y[val_idx], oof[val_idx]))\n",
    "        \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return oof, predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Sun Mar 20 17:09:05 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.53966\tvalid_1's rmse: 3.78742\n",
      "[200]\ttraining's rmse: 3.39621\tvalid_1's rmse: 3.74856\n",
      "[300]\ttraining's rmse: 3.30425\tvalid_1's rmse: 3.73544\n",
      "[400]\ttraining's rmse: 3.23473\tvalid_1's rmse: 3.72852\n",
      "[500]\ttraining's rmse: 3.1792\tvalid_1's rmse: 3.72531\n",
      "[600]\ttraining's rmse: 3.13148\tvalid_1's rmse: 3.72344\n",
      "[700]\ttraining's rmse: 3.0876\tvalid_1's rmse: 3.72284\n",
      "[800]\ttraining's rmse: 3.04712\tvalid_1's rmse: 3.72308\n",
      "[900]\ttraining's rmse: 3.00918\tvalid_1's rmse: 3.72353\n",
      "Early stopping, best iteration is:\n",
      "[667]\ttraining's rmse: 3.10189\tvalid_1's rmse: 3.72259\n",
      "[-0.3714682  -0.08847166 -0.13364805 ...  0.13672201 -0.65193587\n",
      "  0.02118456]\n",
      "Fold 1 started at Sun Mar 20 17:12:20 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.57064\tvalid_1's rmse: 3.65723\n",
      "[200]\ttraining's rmse: 3.42429\tvalid_1's rmse: 3.62102\n",
      "[300]\ttraining's rmse: 3.33379\tvalid_1's rmse: 3.60812\n",
      "[400]\ttraining's rmse: 3.26777\tvalid_1's rmse: 3.60128\n",
      "[500]\ttraining's rmse: 3.21216\tvalid_1's rmse: 3.59793\n",
      "[600]\ttraining's rmse: 3.16296\tvalid_1's rmse: 3.59641\n",
      "[700]\ttraining's rmse: 3.12041\tvalid_1's rmse: 3.59581\n",
      "[800]\ttraining's rmse: 3.0805\tvalid_1's rmse: 3.59551\n",
      "[900]\ttraining's rmse: 3.04422\tvalid_1's rmse: 3.59623\n",
      "[1000]\ttraining's rmse: 3.00983\tvalid_1's rmse: 3.59594\n",
      "Early stopping, best iteration is:\n",
      "[751]\ttraining's rmse: 3.09939\tvalid_1's rmse: 3.59516\n",
      "[-0.79718611 -0.14989229 -0.27157195 ...  0.2917547  -1.33019273\n",
      "  0.0163467 ]\n",
      "Fold 2 started at Sun Mar 20 17:15:08 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.56234\tvalid_1's rmse: 3.69763\n",
      "[200]\ttraining's rmse: 3.41614\tvalid_1's rmse: 3.66449\n",
      "[300]\ttraining's rmse: 3.32268\tvalid_1's rmse: 3.65289\n",
      "[400]\ttraining's rmse: 3.25588\tvalid_1's rmse: 3.64783\n",
      "[500]\ttraining's rmse: 3.20147\tvalid_1's rmse: 3.64444\n",
      "[600]\ttraining's rmse: 3.154\tvalid_1's rmse: 3.64325\n",
      "[700]\ttraining's rmse: 3.11091\tvalid_1's rmse: 3.6422\n",
      "[800]\ttraining's rmse: 3.07252\tvalid_1's rmse: 3.64196\n",
      "[900]\ttraining's rmse: 3.0351\tvalid_1's rmse: 3.64139\n",
      "[1000]\ttraining's rmse: 3.00139\tvalid_1's rmse: 3.64219\n",
      "[1100]\ttraining's rmse: 2.96868\tvalid_1's rmse: 3.6429\n",
      "Early stopping, best iteration is:\n",
      "[897]\ttraining's rmse: 3.0364\tvalid_1's rmse: 3.64129\n",
      "[-1.21352807 -0.20190388 -0.4437285  ...  0.44687097 -1.94202991\n",
      "  0.03884396]\n",
      "Fold 3 started at Sun Mar 20 17:18:08 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.5825\tvalid_1's rmse: 3.59974\n",
      "[200]\ttraining's rmse: 3.4371\tvalid_1's rmse: 3.56997\n",
      "[300]\ttraining's rmse: 3.34527\tvalid_1's rmse: 3.56187\n",
      "[400]\ttraining's rmse: 3.27649\tvalid_1's rmse: 3.55815\n",
      "[500]\ttraining's rmse: 3.21924\tvalid_1's rmse: 3.55633\n",
      "[600]\ttraining's rmse: 3.17062\tvalid_1's rmse: 3.55546\n",
      "[700]\ttraining's rmse: 3.12708\tvalid_1's rmse: 3.55582\n",
      "[800]\ttraining's rmse: 3.08765\tvalid_1's rmse: 3.55646\n",
      "Early stopping, best iteration is:\n",
      "[585]\ttraining's rmse: 3.17767\tvalid_1's rmse: 3.55527\n",
      "[-1.70474423 -0.24653266 -0.56520761 ...  0.60294303 -2.73947949\n",
      "  0.07061852]\n",
      "Fold 4 started at Sun Mar 20 17:20:47 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.54684\tvalid_1's rmse: 3.76046\n",
      "[200]\ttraining's rmse: 3.40143\tvalid_1's rmse: 3.72221\n",
      "[300]\ttraining's rmse: 3.30936\tvalid_1's rmse: 3.71004\n",
      "[400]\ttraining's rmse: 3.24217\tvalid_1's rmse: 3.70548\n",
      "[500]\ttraining's rmse: 3.18687\tvalid_1's rmse: 3.70332\n",
      "[600]\ttraining's rmse: 3.13966\tvalid_1's rmse: 3.70255\n",
      "[700]\ttraining's rmse: 3.09582\tvalid_1's rmse: 3.70115\n",
      "[800]\ttraining's rmse: 3.05837\tvalid_1's rmse: 3.70138\n",
      "[900]\ttraining's rmse: 3.02154\tvalid_1's rmse: 3.70126\n",
      "[1000]\ttraining's rmse: 2.98527\tvalid_1's rmse: 3.70149\n",
      "[1100]\ttraining's rmse: 2.95243\tvalid_1's rmse: 3.7015\n",
      "Early stopping, best iteration is:\n",
      "[858]\ttraining's rmse: 3.03667\tvalid_1's rmse: 3.70081\n",
      "[-2.09159323 -0.35784894 -0.77742657 ...  0.77385207 -3.49572159\n",
      "  0.09271974]\n",
      "CV mean score: 3.6430, std: 0.0627.\n"
     ]
    }
   ],
   "source": [
    "#### lgb\n",
    "lgb_params = {'num_leaves': 63,\n",
    "             'min_data_in_leaf': 32, \n",
    "             'objective':'regression',\n",
    "             'max_depth': -1,\n",
    "             'learning_rate': 0.01,\n",
    "             \"min_child_samples\": 20,\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"feature_fraction\": 0.9,\n",
    "             \"bagging_freq\": 1,\n",
    "             \"bagging_fraction\": 0.9 ,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             \"lambda_l1\": 0.1,\n",
    "             \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=4096)\n",
    "X_ntrain = ntrain[fea_cols].values\n",
    "X_train  = train[fea_cols].values\n",
    "X_test   = test[fea_cols].values\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_lgb , predictions_lgb , scores_lgb  = train_model(X_train , X_test, y_train, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Sun Mar 20 18:54:16 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.58504\tvalid_1's rmse: 1.5733\n",
      "[200]\ttraining's rmse: 1.54442\tvalid_1's rmse: 1.54798\n",
      "[300]\ttraining's rmse: 1.52309\tvalid_1's rmse: 1.54028\n",
      "[400]\ttraining's rmse: 1.50746\tvalid_1's rmse: 1.53688\n",
      "[500]\ttraining's rmse: 1.49407\tvalid_1's rmse: 1.53506\n",
      "[600]\ttraining's rmse: 1.482\tvalid_1's rmse: 1.53411\n",
      "[700]\ttraining's rmse: 1.4712\tvalid_1's rmse: 1.53369\n",
      "[800]\ttraining's rmse: 1.46106\tvalid_1's rmse: 1.53352\n",
      "[900]\ttraining's rmse: 1.45147\tvalid_1's rmse: 1.53335\n",
      "[1000]\ttraining's rmse: 1.44226\tvalid_1's rmse: 1.53318\n",
      "[1100]\ttraining's rmse: 1.4334\tvalid_1's rmse: 1.5332\n",
      "[1200]\ttraining's rmse: 1.42479\tvalid_1's rmse: 1.53322\n",
      "[1300]\ttraining's rmse: 1.41631\tvalid_1's rmse: 1.53322\n",
      "[1400]\ttraining's rmse: 1.40783\tvalid_1's rmse: 1.53322\n",
      "[1500]\ttraining's rmse: 1.39966\tvalid_1's rmse: 1.53321\n",
      "Early stopping, best iteration is:\n",
      "[1261]\ttraining's rmse: 1.41949\tvalid_1's rmse: 1.53315\n",
      "[-0.07321263 -0.0271478  -0.09737258 ...  0.1823629  -0.14511807\n",
      "  0.02013157]\n",
      "Fold 1 started at Sun Mar 20 18:58:37 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.57477\tvalid_1's rmse: 1.62109\n",
      "[200]\ttraining's rmse: 1.53489\tvalid_1's rmse: 1.5918\n",
      "[300]\ttraining's rmse: 1.5138\tvalid_1's rmse: 1.58238\n",
      "[400]\ttraining's rmse: 1.49813\tvalid_1's rmse: 1.57837\n",
      "[500]\ttraining's rmse: 1.48482\tvalid_1's rmse: 1.57649\n",
      "[600]\ttraining's rmse: 1.47298\tvalid_1's rmse: 1.57551\n",
      "[700]\ttraining's rmse: 1.46205\tvalid_1's rmse: 1.57497\n",
      "[800]\ttraining's rmse: 1.45215\tvalid_1's rmse: 1.57474\n",
      "[900]\ttraining's rmse: 1.44252\tvalid_1's rmse: 1.57451\n",
      "[1000]\ttraining's rmse: 1.43345\tvalid_1's rmse: 1.57443\n",
      "[1100]\ttraining's rmse: 1.42461\tvalid_1's rmse: 1.57441\n",
      "[1200]\ttraining's rmse: 1.41612\tvalid_1's rmse: 1.57431\n",
      "[1300]\ttraining's rmse: 1.40775\tvalid_1's rmse: 1.57432\n",
      "[1400]\ttraining's rmse: 1.39962\tvalid_1's rmse: 1.57443\n",
      "[1500]\ttraining's rmse: 1.39175\tvalid_1's rmse: 1.57445\n",
      "Early stopping, best iteration is:\n",
      "[1263]\ttraining's rmse: 1.41082\tvalid_1's rmse: 1.57429\n",
      "[-0.13427919 -0.08613952 -0.17176518 ...  0.36771909 -0.26926109\n",
      "  0.02850995]\n",
      "Fold 2 started at Sun Mar 20 19:05:17 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.58249\tvalid_1's rmse: 1.58693\n",
      "[200]\ttraining's rmse: 1.54257\tvalid_1's rmse: 1.55975\n",
      "[300]\ttraining's rmse: 1.52153\tvalid_1's rmse: 1.55122\n",
      "[400]\ttraining's rmse: 1.50599\tvalid_1's rmse: 1.5472\n",
      "[500]\ttraining's rmse: 1.49265\tvalid_1's rmse: 1.54519\n",
      "[600]\ttraining's rmse: 1.48094\tvalid_1's rmse: 1.5441\n",
      "[700]\ttraining's rmse: 1.47016\tvalid_1's rmse: 1.54357\n",
      "[800]\ttraining's rmse: 1.46015\tvalid_1's rmse: 1.54327\n",
      "[900]\ttraining's rmse: 1.45074\tvalid_1's rmse: 1.54331\n",
      "[1000]\ttraining's rmse: 1.44146\tvalid_1's rmse: 1.54306\n",
      "[1100]\ttraining's rmse: 1.43253\tvalid_1's rmse: 1.54296\n",
      "[1200]\ttraining's rmse: 1.42378\tvalid_1's rmse: 1.54284\n",
      "[1300]\ttraining's rmse: 1.41537\tvalid_1's rmse: 1.5427\n",
      "[1400]\ttraining's rmse: 1.40706\tvalid_1's rmse: 1.54277\n",
      "[1500]\ttraining's rmse: 1.39894\tvalid_1's rmse: 1.54285\n",
      "[1600]\ttraining's rmse: 1.39109\tvalid_1's rmse: 1.54291\n",
      "Early stopping, best iteration is:\n",
      "[1306]\ttraining's rmse: 1.41487\tvalid_1's rmse: 1.54268\n",
      "[-0.18097505 -0.13900764 -0.26260062 ...  0.53554396 -0.39582246\n",
      "  0.05366009]\n",
      "Fold 3 started at Sun Mar 20 19:09:39 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.57976\tvalid_1's rmse: 1.59453\n",
      "[200]\ttraining's rmse: 1.53937\tvalid_1's rmse: 1.56883\n",
      "[300]\ttraining's rmse: 1.51805\tvalid_1's rmse: 1.5611\n",
      "[400]\ttraining's rmse: 1.50246\tvalid_1's rmse: 1.55756\n",
      "[500]\ttraining's rmse: 1.48913\tvalid_1's rmse: 1.5556\n",
      "[600]\ttraining's rmse: 1.47706\tvalid_1's rmse: 1.55474\n",
      "[700]\ttraining's rmse: 1.46606\tvalid_1's rmse: 1.5543\n",
      "[800]\ttraining's rmse: 1.45581\tvalid_1's rmse: 1.55416\n",
      "[900]\ttraining's rmse: 1.44614\tvalid_1's rmse: 1.55396\n",
      "[1000]\ttraining's rmse: 1.43691\tvalid_1's rmse: 1.55385\n",
      "[1100]\ttraining's rmse: 1.42814\tvalid_1's rmse: 1.55373\n",
      "[1200]\ttraining's rmse: 1.41946\tvalid_1's rmse: 1.55388\n",
      "[1300]\ttraining's rmse: 1.41105\tvalid_1's rmse: 1.55389\n",
      "Early stopping, best iteration is:\n",
      "[1096]\ttraining's rmse: 1.42846\tvalid_1's rmse: 1.55372\n",
      "[-0.23497504 -0.19660437 -0.34921254 ...  0.69725198 -0.52292008\n",
      "  0.09083277]\n",
      "Fold 4 started at Sun Mar 20 19:13:35 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.58177\tvalid_1's rmse: 1.58912\n",
      "[200]\ttraining's rmse: 1.54163\tvalid_1's rmse: 1.56141\n",
      "[300]\ttraining's rmse: 1.52047\tvalid_1's rmse: 1.55271\n",
      "[400]\ttraining's rmse: 1.50501\tvalid_1's rmse: 1.54881\n",
      "[500]\ttraining's rmse: 1.49181\tvalid_1's rmse: 1.54649\n",
      "[600]\ttraining's rmse: 1.47976\tvalid_1's rmse: 1.54531\n",
      "[700]\ttraining's rmse: 1.46902\tvalid_1's rmse: 1.54472\n",
      "[800]\ttraining's rmse: 1.45877\tvalid_1's rmse: 1.54419\n",
      "[900]\ttraining's rmse: 1.44918\tvalid_1's rmse: 1.54392\n",
      "[1000]\ttraining's rmse: 1.43987\tvalid_1's rmse: 1.54365\n",
      "[1100]\ttraining's rmse: 1.43103\tvalid_1's rmse: 1.5436\n",
      "[1200]\ttraining's rmse: 1.42249\tvalid_1's rmse: 1.54348\n",
      "[1300]\ttraining's rmse: 1.41409\tvalid_1's rmse: 1.54343\n",
      "[1400]\ttraining's rmse: 1.40566\tvalid_1's rmse: 1.54353\n",
      "[1500]\ttraining's rmse: 1.39758\tvalid_1's rmse: 1.54346\n",
      "Early stopping, best iteration is:\n",
      "[1267]\ttraining's rmse: 1.41684\tvalid_1's rmse: 1.54342\n",
      "[-0.30087792 -0.26634918 -0.42404652 ...  0.88054725 -0.6636932\n",
      "  0.11616671]\n",
      "CV mean score: 1.5495, std: 0.0140.\n"
     ]
    }
   ],
   "source": [
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nlgb, predictions_nlgb, scores_nlgb = train_model(X_ntrain, X_test, y_ntrain, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 分类模型 ==========\n",
      "Fold 0 started at Sun Mar 20 19:29:42 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0342044\tvalid_1's binary_logloss: 0.0493479\n",
      "[200]\ttraining's binary_logloss: 0.0268654\tvalid_1's binary_logloss: 0.0473549\n",
      "[300]\ttraining's binary_logloss: 0.022339\tvalid_1's binary_logloss: 0.0469053\n",
      "[400]\ttraining's binary_logloss: 0.019187\tvalid_1's binary_logloss: 0.0468784\n",
      "[500]\ttraining's binary_logloss: 0.0167221\tvalid_1's binary_logloss: 0.0470381\n",
      "[600]\ttraining's binary_logloss: 0.0146748\tvalid_1's binary_logloss: 0.047282\n",
      "Early stopping, best iteration is:\n",
      "[347]\ttraining's binary_logloss: 0.0207395\tvalid_1's binary_logloss: 0.0468522\n",
      "[0.00936449 0.00024249 0.0017062  ... 0.00134461 0.01087633 0.00060872]\n",
      "Fold 1 started at Sun Mar 20 19:31:54 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0350399\tvalid_1's binary_logloss: 0.0457612\n",
      "[200]\ttraining's binary_logloss: 0.0275558\tvalid_1's binary_logloss: 0.0437777\n",
      "[300]\ttraining's binary_logloss: 0.0229278\tvalid_1's binary_logloss: 0.0432826\n",
      "[400]\ttraining's binary_logloss: 0.0197749\tvalid_1's binary_logloss: 0.0431588\n",
      "[500]\ttraining's binary_logloss: 0.0172632\tvalid_1's binary_logloss: 0.0432853\n",
      "[600]\ttraining's binary_logloss: 0.0152053\tvalid_1's binary_logloss: 0.0434253\n",
      "Early stopping, best iteration is:\n",
      "[378]\ttraining's binary_logloss: 0.0204135\tvalid_1's binary_logloss: 0.0431552\n",
      "[0.01740865 0.00052304 0.00407929 ... 0.00254597 0.0210323  0.00116679]\n",
      "Fold 2 started at Sun Mar 20 19:49:19 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0348582\tvalid_1's binary_logloss: 0.0468198\n",
      "[200]\ttraining's binary_logloss: 0.0273993\tvalid_1's binary_logloss: 0.0449155\n",
      "[300]\ttraining's binary_logloss: 0.0228235\tvalid_1's binary_logloss: 0.0443797\n",
      "[400]\ttraining's binary_logloss: 0.0196376\tvalid_1's binary_logloss: 0.0442406\n",
      "[500]\ttraining's binary_logloss: 0.0171338\tvalid_1's binary_logloss: 0.044312\n",
      "[600]\ttraining's binary_logloss: 0.0150678\tvalid_1's binary_logloss: 0.0444947\n",
      "Early stopping, best iteration is:\n",
      "[374]\ttraining's binary_logloss: 0.0204026\tvalid_1's binary_logloss: 0.0442324\n",
      "[0.02393862 0.00077037 0.00645539 ... 0.00390537 0.03161608 0.00183872]\n",
      "Fold 3 started at Sun Mar 20 19:51:24 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0355635\tvalid_1's binary_logloss: 0.0437418\n",
      "[200]\ttraining's binary_logloss: 0.0279059\tvalid_1's binary_logloss: 0.041912\n",
      "[300]\ttraining's binary_logloss: 0.0232765\tvalid_1's binary_logloss: 0.0414079\n",
      "[400]\ttraining's binary_logloss: 0.0200649\tvalid_1's binary_logloss: 0.041307\n",
      "[500]\ttraining's binary_logloss: 0.0175178\tvalid_1's binary_logloss: 0.0413924\n",
      "[600]\ttraining's binary_logloss: 0.0154402\tvalid_1's binary_logloss: 0.0416239\n",
      "[700]\ttraining's binary_logloss: 0.0136594\tvalid_1's binary_logloss: 0.04184\n",
      "Early stopping, best iteration is:\n",
      "[415]\ttraining's binary_logloss: 0.0196565\tvalid_1's binary_logloss: 0.0413014\n",
      "[0.03227355 0.00104602 0.00734491 ... 0.00523079 0.044427   0.00222399]\n",
      "Fold 4 started at Sun Mar 20 19:53:46 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0343898\tvalid_1's binary_logloss: 0.0482267\n",
      "[200]\ttraining's binary_logloss: 0.0270262\tvalid_1's binary_logloss: 0.0462438\n",
      "[300]\ttraining's binary_logloss: 0.0224465\tvalid_1's binary_logloss: 0.0456764\n",
      "[400]\ttraining's binary_logloss: 0.0193184\tvalid_1's binary_logloss: 0.0455848\n",
      "[500]\ttraining's binary_logloss: 0.0168514\tvalid_1's binary_logloss: 0.0457501\n",
      "[600]\ttraining's binary_logloss: 0.0148183\tvalid_1's binary_logloss: 0.0460262\n",
      "Early stopping, best iteration is:\n",
      "[372]\ttraining's binary_logloss: 0.0201184\tvalid_1's binary_logloss: 0.0455742\n",
      "[0.03932077 0.00135798 0.00946336 ... 0.00638885 0.056854   0.00274857]\n",
      "CV mean score: 0.0442, std: 0.0019.\n"
     ]
    }
   ],
   "source": [
    "print('='*10,'分类模型','='*10)\n",
    "lgb_params['objective'] = 'binary'\n",
    "lgb_params['metric']    = 'binary_logloss'\n",
    "oof_blgb, predictions_blgb, scores_blgb = train_model(X_train , X_test, y_train_binary, params=lgb_params, folds=folds, model_type='lgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../../data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_lgb\n",
    "sub_df.to_csv('predictions_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb  = pd.DataFrame(oof_lgb)\n",
    "oof_nlgb = pd.DataFrame(oof_nlgb)\n",
    "oof_blgb = pd.DataFrame(oof_blgb)\n",
    "\n",
    "predictions_lgb  = pd.DataFrame(predictions_lgb)\n",
    "predictions_nlgb = pd.DataFrame(predictions_nlgb)\n",
    "predictions_blgb = pd.DataFrame(predictions_blgb)\n",
    "\n",
    "oof_lgb.to_csv('../../result/oof_lgb.csv',header=None,index=False)\n",
    "oof_blgb.to_csv('../../result/oof_blgb.csv',header=None,index=False)\n",
    "oof_nlgb.to_csv('../../result/oof_nlgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_lgb.to_csv('../../result/predictions_lgb.csv',header=None,index=False)\n",
    "predictions_nlgb.to_csv('../../result/predictions_nlgb.csv',header=None,index=False)\n",
    "predictions_blgb.to_csv('../../result/predictions_blgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Sun Mar 20 19:57:45 2022\n",
      "[19:57:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:57:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.90088\tvalid_data-rmse:4.07156\n",
      "[100]\ttrain-rmse:3.11743\tvalid_data-rmse:3.79847\n",
      "[200]\ttrain-rmse:2.96223\tvalid_data-rmse:3.79919\n",
      "[286]\ttrain-rmse:2.84050\tvalid_data-rmse:3.80300\n",
      "[-0.19234598 -0.02435767 -0.15576263 ...  0.06688782 -0.16866477\n",
      "  0.00856968]\n",
      "Fold 1 started at Sun Mar 20 20:19:53 2022\n",
      "[20:19:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:19:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.93080\tvalid_data-rmse:3.79335\n",
      "[100]\ttrain-rmse:3.14149\tvalid_data-rmse:3.55781\n",
      "[200]\ttrain-rmse:2.97706\tvalid_data-rmse:3.56277\n",
      "[247]\ttrain-rmse:2.91367\tvalid_data-rmse:3.56544\n",
      "[-0.32142258 -0.04752996 -0.22262365 ...  0.12088079 -0.43355958\n",
      " -0.02616672]\n",
      "Fold 2 started at Sun Mar 20 20:54:39 2022\n",
      "[20:54:42] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:54:42] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.93248\tvalid_data-rmse:3.78781\n",
      "[100]\ttrain-rmse:3.13313\tvalid_data-rmse:3.54400\n",
      "[200]\ttrain-rmse:2.96312\tvalid_data-rmse:3.54750\n",
      "[258]\ttrain-rmse:2.87498\tvalid_data-rmse:3.55328\n",
      "[-0.65371609 -0.07871552 -0.30649623 ...  0.20723865 -0.72761951\n",
      " -0.00785216]\n",
      "Fold 3 started at Sun Mar 20 21:40:56 2022\n",
      "[21:41:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:41:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.91039\tvalid_data-rmse:3.97625\n",
      "[100]\ttrain-rmse:3.12567\tvalid_data-rmse:3.68917\n",
      "[200]\ttrain-rmse:2.95762\tvalid_data-rmse:3.69617\n",
      "[300]\ttrain-rmse:2.79724\tvalid_data-rmse:3.70490\n",
      "[307]\ttrain-rmse:2.78819\tvalid_data-rmse:3.70525\n",
      "[-0.76090238 -0.12735682 -0.36758466 ...  0.27665785 -1.16888301\n",
      "  0.01659006]\n",
      "Fold 4 started at Mon Mar 21 02:44:34 2022\n",
      "[02:44:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:44:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.90884\tvalid_data-rmse:3.99879\n",
      "[100]\ttrain-rmse:3.12992\tvalid_data-rmse:3.69149\n",
      "[200]\ttrain-rmse:2.96101\tvalid_data-rmse:3.69387\n",
      "[300]\ttrain-rmse:2.81621\tvalid_data-rmse:3.69865\n",
      "[334]\ttrain-rmse:2.76940\tvalid_data-rmse:3.69972\n",
      "[-0.94568162 -0.14572568 -0.50795638 ...  0.36239104 -1.58373539\n",
      "  0.04459268]\n",
      "Fold 5 started at Mon Mar 21 08:24:58 2022\n",
      "[08:25:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:25:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.92510\tvalid_data-rmse:3.84945\n",
      "[100]\ttrain-rmse:3.13254\tvalid_data-rmse:3.58266\n",
      "[200]\ttrain-rmse:2.97854\tvalid_data-rmse:3.58637\n",
      "[269]\ttrain-rmse:2.87649\tvalid_data-rmse:3.59213\n",
      "[-1.02943225 -0.21540624 -0.56666461 ...  0.42956803 -1.79817225\n",
      "  0.04367594]\n",
      "Fold 6 started at Mon Mar 21 08:44:22 2022\n",
      "[08:44:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:44:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.91615\tvalid_data-rmse:3.92370\n",
      "[100]\ttrain-rmse:3.13944\tvalid_data-rmse:3.64704\n",
      "[200]\ttrain-rmse:2.97930\tvalid_data-rmse:3.65648\n",
      "[267]\ttrain-rmse:2.88167\tvalid_data-rmse:3.66356\n",
      "[-1.21934341 -0.23452545 -0.7061008  ...  0.51098562 -2.26920708\n",
      "  0.04912403]\n",
      "Fold 7 started at Mon Mar 21 09:03:13 2022\n",
      "[09:03:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:03:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.91905\tvalid_data-rmse:3.90177\n",
      "[100]\ttrain-rmse:3.15318\tvalid_data-rmse:3.59261\n",
      "[200]\ttrain-rmse:2.97787\tvalid_data-rmse:3.59587\n",
      "[300]\ttrain-rmse:2.83752\tvalid_data-rmse:3.60000\n",
      "[301]\ttrain-rmse:2.83677\tvalid_data-rmse:3.60011\n",
      "[-1.33667192 -0.25589069 -0.84005637 ...  0.55636062 -2.55980842\n",
      "  0.0393764 ]\n",
      "Fold 8 started at Mon Mar 21 09:22:03 2022\n",
      "[09:22:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:22:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.92518\tvalid_data-rmse:3.86761\n",
      "[100]\ttrain-rmse:3.15725\tvalid_data-rmse:3.60955\n",
      "[200]\ttrain-rmse:2.99974\tvalid_data-rmse:3.61399\n",
      "[249]\ttrain-rmse:2.92673\tvalid_data-rmse:3.61980\n",
      "[-1.44170471 -0.27301808 -0.89988858 ...  0.62205714 -2.73579977\n",
      "  0.05130264]\n",
      "Fold 9 started at Mon Mar 21 09:37:33 2022\n",
      "[09:37:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:37:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.89793\tvalid_data-rmse:4.08693\n",
      "[100]\ttrain-rmse:3.10986\tvalid_data-rmse:3.80008\n",
      "[200]\ttrain-rmse:2.94116\tvalid_data-rmse:3.79921\n",
      "[300]\ttrain-rmse:2.80240\tvalid_data-rmse:3.80451\n",
      "[391]\ttrain-rmse:2.68892\tvalid_data-rmse:3.80260\n",
      "[-1.69788344 -0.31066354 -0.94600077 ...  0.66025437 -2.92991857\n",
      "  0.05219223]\n",
      "CV mean score: 3.6491, std: 0.0883.\n",
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Mon Mar 21 10:03:44 2022\n",
      "[10:03:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:03:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77493\tvalid_data-rmse:1.76901\n",
      "[100]\ttrain-rmse:1.36329\tvalid_data-rmse:1.54528\n",
      "[200]\ttrain-rmse:1.30348\tvalid_data-rmse:1.54593\n",
      "[294]\ttrain-rmse:1.24981\tvalid_data-rmse:1.54714\n",
      "[-0.03302843 -0.02536802 -0.08496726 ...  0.07956059 -0.05550308\n",
      "  0.03351417]\n",
      "Fold 1 started at Mon Mar 21 10:24:44 2022\n",
      "[10:24:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:24:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77310\tvalid_data-rmse:1.78636\n",
      "[100]\ttrain-rmse:1.36101\tvalid_data-rmse:1.56373\n",
      "[200]\ttrain-rmse:1.29789\tvalid_data-rmse:1.56424\n",
      "[290]\ttrain-rmse:1.24885\tvalid_data-rmse:1.56566\n",
      "[-0.04883505 -0.04490577 -0.14848059 ...  0.133423   -0.11574998\n",
      "  0.04465394]\n",
      "Fold 2 started at Mon Mar 21 10:47:36 2022\n",
      "[10:47:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:47:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77410\tvalid_data-rmse:1.77862\n",
      "[100]\ttrain-rmse:1.36468\tvalid_data-rmse:1.55760\n",
      "[200]\ttrain-rmse:1.30459\tvalid_data-rmse:1.55890\n",
      "[300]\ttrain-rmse:1.25004\tvalid_data-rmse:1.56158\n",
      "[329]\ttrain-rmse:1.23316\tvalid_data-rmse:1.56208\n",
      "[-0.08710754 -0.05955591 -0.19295745 ...  0.22526322 -0.18161576\n",
      "  0.06739594]\n",
      "Fold 3 started at Mon Mar 21 11:11:56 2022\n",
      "[11:12:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:12:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77427\tvalid_data-rmse:1.77451\n",
      "[100]\ttrain-rmse:1.36044\tvalid_data-rmse:1.54577\n",
      "[200]\ttrain-rmse:1.29530\tvalid_data-rmse:1.54621\n",
      "[300]\ttrain-rmse:1.23849\tvalid_data-rmse:1.54868\n",
      "[329]\ttrain-rmse:1.22319\tvalid_data-rmse:1.54907\n",
      "[-0.13829672 -0.08143756 -0.26333179 ...  0.27999776 -0.2646831\n",
      "  0.08145732]\n",
      "Fold 4 started at Mon Mar 21 12:09:05 2022\n",
      "[12:09:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:09:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77645\tvalid_data-rmse:1.75325\n",
      "[100]\ttrain-rmse:1.36732\tvalid_data-rmse:1.53888\n",
      "[200]\ttrain-rmse:1.30836\tvalid_data-rmse:1.53988\n",
      "[292]\ttrain-rmse:1.25605\tvalid_data-rmse:1.54110\n",
      "[-0.16719642 -0.10799123 -0.30961044 ...  0.37318326 -0.31342421\n",
      "  0.10167168]\n",
      "Fold 5 started at Mon Mar 21 13:45:34 2022\n",
      "[13:45:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:45:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77370\tvalid_data-rmse:1.78131\n",
      "[100]\ttrain-rmse:1.36248\tvalid_data-rmse:1.55330\n",
      "[200]\ttrain-rmse:1.29692\tvalid_data-rmse:1.55354\n",
      "[300]\ttrain-rmse:1.24233\tvalid_data-rmse:1.55473\n",
      "[323]\ttrain-rmse:1.22990\tvalid_data-rmse:1.55486\n",
      "[-0.19591467 -0.11622516 -0.35699658 ...  0.48561326 -0.38493592\n",
      "  0.11504208]\n",
      "Fold 6 started at Mon Mar 21 14:06:26 2022\n",
      "[14:06:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:06:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77293\tvalid_data-rmse:1.78904\n",
      "[100]\ttrain-rmse:1.36279\tvalid_data-rmse:1.55447\n",
      "[200]\ttrain-rmse:1.30050\tvalid_data-rmse:1.55466\n",
      "[300]\ttrain-rmse:1.24270\tvalid_data-rmse:1.55604\n",
      "[355]\ttrain-rmse:1.21373\tvalid_data-rmse:1.55648\n",
      "[-0.2443529  -0.13548484 -0.4184189  ...  0.55888994 -0.45166326\n",
      "  0.1367524 ]\n",
      "Fold 7 started at Mon Mar 21 14:28:28 2022\n",
      "[14:28:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:28:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77160\tvalid_data-rmse:1.79806\n",
      "[100]\ttrain-rmse:1.36303\tvalid_data-rmse:1.56631\n",
      "[200]\ttrain-rmse:1.30176\tvalid_data-rmse:1.56579\n",
      "[300]\ttrain-rmse:1.24730\tvalid_data-rmse:1.56629\n",
      "[400]\ttrain-rmse:1.19321\tvalid_data-rmse:1.56826\n",
      "[425]\ttrain-rmse:1.17972\tvalid_data-rmse:1.56841\n",
      "[-0.27973769 -0.13804098 -0.50039741 ...  0.6529497  -0.51546075\n",
      "  0.1567915 ]\n",
      "Fold 8 started at Mon Mar 21 15:45:16 2022\n",
      "[15:45:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:45:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77686\tvalid_data-rmse:1.75369\n",
      "[100]\ttrain-rmse:1.36727\tvalid_data-rmse:1.53239\n",
      "[200]\ttrain-rmse:1.30425\tvalid_data-rmse:1.53263\n",
      "[300]\ttrain-rmse:1.24581\tvalid_data-rmse:1.53386\n",
      "[338]\ttrain-rmse:1.22362\tvalid_data-rmse:1.53389\n",
      "[-0.30356734 -0.14663796 -0.57889451 ...  0.74229007 -0.58392898\n",
      "  0.19564462]\n",
      "Fold 9 started at Mon Mar 21 16:09:47 2022\n",
      "[16:09:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:09:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77266\tvalid_data-rmse:1.78987\n",
      "[100]\ttrain-rmse:1.35949\tvalid_data-rmse:1.56703\n",
      "[200]\ttrain-rmse:1.29369\tvalid_data-rmse:1.56725\n",
      "[300]\ttrain-rmse:1.23791\tvalid_data-rmse:1.56948\n",
      "[348]\ttrain-rmse:1.20893\tvalid_data-rmse:1.56952\n",
      "[-0.31861995 -0.14579851 -0.64527602 ...  0.83562773 -0.61904194\n",
      "  0.23378382]\n",
      "CV mean score: 1.5519, std: 0.0111.\n",
      "========== 分类模型 ==========\n",
      "Fold 0 started at Mon Mar 21 16:44:51 2022\n",
      "[16:44:54] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47594\tvalid_data-rmse:0.47613\n",
      "[100]\ttrain-rmse:0.09498\tvalid_data-rmse:0.10399\n",
      "[200]\ttrain-rmse:0.09057\tvalid_data-rmse:0.10405\n",
      "[300]\ttrain-rmse:0.08608\tvalid_data-rmse:0.10410\n",
      "[324]\ttrain-rmse:0.08492\tvalid_data-rmse:0.10416\n",
      "[0.0044455  0.00035132 0.00083342 ... 0.00093245 0.00647907 0.00065936]\n",
      "Fold 1 started at Mon Mar 21 17:01:25 2022\n",
      "[17:01:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47594\tvalid_data-rmse:0.47605\n",
      "[100]\ttrain-rmse:0.09569\tvalid_data-rmse:0.09623\n",
      "[200]\ttrain-rmse:0.09119\tvalid_data-rmse:0.09606\n",
      "[300]\ttrain-rmse:0.08649\tvalid_data-rmse:0.09627\n",
      "[392]\ttrain-rmse:0.08227\tvalid_data-rmse:0.09639\n",
      "[0.00809478 0.00062717 0.00229079 ... 0.00168229 0.01564699 0.00100111]\n",
      "Fold 2 started at Mon Mar 21 17:27:04 2022\n",
      "[17:27:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47609\tvalid_data-rmse:0.47602\n",
      "[100]\ttrain-rmse:0.09572\tvalid_data-rmse:0.09563\n",
      "[200]\ttrain-rmse:0.09118\tvalid_data-rmse:0.09602\n",
      "[300]\ttrain-rmse:0.08654\tvalid_data-rmse:0.09604\n",
      "[305]\ttrain-rmse:0.08632\tvalid_data-rmse:0.09602\n",
      "[0.01373569 0.00103815 0.00347926 ... 0.0027072  0.02520931 0.00178706]\n",
      "Fold 3 started at Mon Mar 21 17:48:44 2022\n",
      "[17:48:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47593\tvalid_data-rmse:0.47611\n",
      "[100]\ttrain-rmse:0.09537\tvalid_data-rmse:0.10034\n",
      "[200]\ttrain-rmse:0.09100\tvalid_data-rmse:0.10045\n",
      "[300]\ttrain-rmse:0.08640\tvalid_data-rmse:0.10060\n",
      "[342]\ttrain-rmse:0.08454\tvalid_data-rmse:0.10075\n",
      "[0.01798994 0.00125493 0.00482792 ... 0.00357808 0.03188393 0.00224982]\n",
      "Fold 4 started at Mon Mar 21 18:04:45 2022\n",
      "[18:04:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47593\tvalid_data-rmse:0.47608\n",
      "[100]\ttrain-rmse:0.09537\tvalid_data-rmse:0.10091\n",
      "[200]\ttrain-rmse:0.09097\tvalid_data-rmse:0.10084\n",
      "[300]\ttrain-rmse:0.08620\tvalid_data-rmse:0.10113\n",
      "[338]\ttrain-rmse:0.08453\tvalid_data-rmse:0.10115\n",
      "[0.02302127 0.00153878 0.00625948 ... 0.00446372 0.03946579 0.00288803]\n",
      "Fold 5 started at Mon Mar 21 18:20:10 2022\n",
      "[18:20:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47595\tvalid_data-rmse:0.47606\n",
      "[100]\ttrain-rmse:0.09567\tvalid_data-rmse:0.09707\n",
      "[200]\ttrain-rmse:0.09132\tvalid_data-rmse:0.09687\n",
      "[300]\ttrain-rmse:0.08680\tvalid_data-rmse:0.09687\n",
      "[400]\ttrain-rmse:0.08234\tvalid_data-rmse:0.09685\n",
      "[447]\ttrain-rmse:0.08024\tvalid_data-rmse:0.09692\n",
      "[0.02584941 0.00170482 0.00779309 ... 0.00542453 0.04550367 0.00329329]\n",
      "Fold 6 started at Mon Mar 21 18:41:05 2022\n",
      "[18:41:08] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47594\tvalid_data-rmse:0.47608\n",
      "[100]\ttrain-rmse:0.09557\tvalid_data-rmse:0.09906\n",
      "[200]\ttrain-rmse:0.09128\tvalid_data-rmse:0.09927\n",
      "[297]\ttrain-rmse:0.08682\tvalid_data-rmse:0.09952\n",
      "[0.03181856 0.00225875 0.00923426 ... 0.00669855 0.05310553 0.0042704 ]\n",
      "Fold 7 started at Mon Mar 21 18:54:39 2022\n",
      "[18:54:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47613\tvalid_data-rmse:0.47606\n",
      "[100]\ttrain-rmse:0.09573\tvalid_data-rmse:0.09778\n",
      "[200]\ttrain-rmse:0.09132\tvalid_data-rmse:0.09763\n",
      "[300]\ttrain-rmse:0.08675\tvalid_data-rmse:0.09790\n",
      "[400]\ttrain-rmse:0.08226\tvalid_data-rmse:0.09818\n",
      "[424]\ttrain-rmse:0.08125\tvalid_data-rmse:0.09824\n",
      "[0.03466113 0.00243615 0.0103015  ... 0.00761834 0.06151151 0.00467712]\n",
      "Fold 8 started at Mon Mar 21 19:17:40 2022\n",
      "[19:17:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47594\tvalid_data-rmse:0.47605\n",
      "[100]\ttrain-rmse:0.09573\tvalid_data-rmse:0.09770\n",
      "[200]\ttrain-rmse:0.09141\tvalid_data-rmse:0.09758\n",
      "[300]\ttrain-rmse:0.08664\tvalid_data-rmse:0.09778\n",
      "[400]\ttrain-rmse:0.08204\tvalid_data-rmse:0.09801\n",
      "[402]\ttrain-rmse:0.08195\tvalid_data-rmse:0.09800\n",
      "[0.03793357 0.00270228 0.01202663 ... 0.00874169 0.06808813 0.00513603]\n",
      "Fold 9 started at Mon Mar 21 19:47:50 2022\n",
      "[19:47:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47592\tvalid_data-rmse:0.47615\n",
      "[100]\ttrain-rmse:0.09501\tvalid_data-rmse:0.10412\n",
      "[200]\ttrain-rmse:0.09064\tvalid_data-rmse:0.10397\n",
      "[300]\ttrain-rmse:0.08612\tvalid_data-rmse:0.10406\n",
      "[400]\ttrain-rmse:0.08160\tvalid_data-rmse:0.10426\n",
      "[421]\ttrain-rmse:0.08056\tvalid_data-rmse:0.10431\n",
      "[0.04217812 0.00307473 0.01320962 ... 0.00934906 0.07717252 0.00557381]\n",
      "CV mean score: 0.0442, std: 0.0022.\n"
     ]
    }
   ],
   "source": [
    "#### xgb\n",
    "xgb_params = {'eta':0.05, 'max_leaves':47, 'max_depth':10, 'subsample':0.8, 'colsample_bytree':0.8,\n",
    "              'min_child_weight':40, 'max_bin':128, 'reg_alpha':2.0, 'reg_lambda':2.0, \n",
    "              'objective':'reg:linear', 'eval_metric':'rmse', 'silent': True, 'nthread':4}\n",
    "folds = KFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_xgb , predictions_xgb , scores_xgb  = train_model(X_train , X_test, y_train , params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nxgb, predictions_nxgb, scores_nxgb = train_model(X_ntrain, X_test, y_ntrain, params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "xgb_params['objective'] = 'binary:logistic'\n",
    "xgb_params['metric']    = 'binary_logloss'\n",
    "oof_bxgb, predictions_bxgb, scores_bxgb = train_model(X_train , X_test, y_train_binary, params=xgb_params, folds=folds, model_type='xgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../../data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_xgb\n",
    "sub_df.to_csv('../../predictions_xgb.csv', index=False)\n",
    "\n",
    "oof_xgb  = pd.DataFrame(oof_xgb)\n",
    "oof_nxgb = pd.DataFrame(oof_nxgb)\n",
    "oof_bxgb = pd.DataFrame(oof_bxgb)\n",
    "\n",
    "predictions_xgb  = pd.DataFrame(predictions_xgb)\n",
    "predictions_nxgb = pd.DataFrame(predictions_nxgb)\n",
    "predictions_bxgb = pd.DataFrame(predictions_bxgb)\n",
    "\n",
    "oof_xgb.to_csv('../../result/oof_xgb.csv',header=None,index=False)\n",
    "oof_bxgb.to_csv('../../result/oof_bxgb.csv',header=None,index=False)\n",
    "oof_nxgb.to_csv('../../result/oof_nxgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_xgb.to_csv('../../result/predictions_xgb.csv',header=None,index=False)\n",
    "predictions_nxgb.to_csv('../../result/predictions_nxgb.csv',header=None,index=False)\n",
    "predictions_bxgb.to_csv('../../result/predictions_bxgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Mon Mar 21 22:12:00 2022\n",
      "0:\tlearn: 3.8322414\ttest: 3.8757204\tbest: 3.8757204 (0)\ttotal: 518ms\tremaining: 2h 52m 47s\n",
      "100:\tlearn: 3.5854293\ttest: 3.6786430\tbest: 3.6786430 (100)\ttotal: 15.1s\tremaining: 49m 25s\n",
      "200:\tlearn: 3.5366978\ttest: 3.6671911\tbest: 3.6671911 (200)\ttotal: 29.6s\tremaining: 48m 35s\n",
      "300:\tlearn: 3.5036971\ttest: 3.6633983\tbest: 3.6633417 (293)\ttotal: 44.1s\tremaining: 48m 4s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.66227846\n",
      "bestIteration = 344\n",
      "\n",
      "Shrink model to first 345 iterations.\n",
      "[-0.336061   -0.03190832 -0.09629481 ...  0.09041586 -0.31964174\n",
      "  0.01571148]\n",
      "Fold 1 started at Mon Mar 21 22:13:47 2022\n",
      "0:\tlearn: 3.8423150\ttest: 3.7859420\tbest: 3.7859420 (0)\ttotal: 433ms\tremaining: 2h 24m 23s\n",
      "100:\tlearn: 3.5946278\ttest: 3.6029010\tbest: 3.6029010 (100)\ttotal: 16.1s\tremaining: 52m 49s\n",
      "200:\tlearn: 3.5457570\ttest: 3.5949553\tbest: 3.5949553 (200)\ttotal: 29.9s\tremaining: 49m 5s\n",
      "300:\tlearn: 3.5111864\ttest: 3.5904780\tbest: 3.5904780 (300)\ttotal: 43.4s\tremaining: 47m 18s\n",
      "400:\tlearn: 3.4804531\ttest: 3.5897360\tbest: 3.5896113 (397)\ttotal: 58.1s\tremaining: 47m 19s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.589586472\n",
      "bestIteration = 414\n",
      "\n",
      "Shrink model to first 415 iterations.\n",
      "[-0.73918252 -0.06885808 -0.16332128 ...  0.16771065 -0.66791654\n",
      "  0.03947168]\n",
      "Fold 2 started at Mon Mar 21 22:15:39 2022\n",
      "0:\tlearn: 3.8178743\ttest: 4.0135852\tbest: 4.0135852 (0)\ttotal: 524ms\tremaining: 2h 54m 48s\n",
      "100:\tlearn: 3.5702752\ttest: 3.8169674\tbest: 3.8169640 (99)\ttotal: 14.7s\tremaining: 48m 19s\n",
      "200:\tlearn: 3.5230840\ttest: 3.8066142\tbest: 3.8066142 (200)\ttotal: 27.9s\tremaining: 45m 43s\n",
      "300:\tlearn: 3.4980143\ttest: 3.8036966\tbest: 3.8036189 (293)\ttotal: 40.1s\tremaining: 43m 47s\n",
      "400:\tlearn: 3.4702184\ttest: 3.8014659\tbest: 3.8014204 (388)\ttotal: 52.7s\tremaining: 42m 56s\n",
      "500:\tlearn: 3.4359263\ttest: 3.7985826\tbest: 3.7985826 (500)\ttotal: 1m 5s\tremaining: 42m 23s\n",
      "600:\tlearn: 3.4002455\ttest: 3.7975495\tbest: 3.7971833 (587)\ttotal: 1m 18s\tremaining: 42m 5s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.797183264\n",
      "bestIteration = 587\n",
      "\n",
      "Shrink model to first 588 iterations.\n",
      "[-1.02266776 -0.14210272 -0.26433892 ...  0.25421082 -1.00402518\n",
      "  0.057415  ]\n",
      "Fold 3 started at Mon Mar 21 22:17:48 2022\n",
      "0:\tlearn: 3.8565488\ttest: 3.6608947\tbest: 3.6608947 (0)\ttotal: 438ms\tremaining: 2h 26m 5s\n",
      "100:\tlearn: 3.5955761\ttest: 3.5147224\tbest: 3.5147224 (100)\ttotal: 16.8s\tremaining: 55m 3s\n",
      "200:\tlearn: 3.5497400\ttest: 3.5101687\tbest: 3.5100666 (197)\ttotal: 30.7s\tremaining: 50m 27s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.509766567\n",
      "bestIteration = 207\n",
      "\n",
      "Shrink model to first 208 iterations.\n",
      "[-1.3456504  -0.16938349 -0.33546087 ...  0.34960206 -1.3202266\n",
      "  0.06786926]\n",
      "Fold 4 started at Mon Mar 21 22:22:00 2022\n",
      "0:\tlearn: 3.8437309\ttest: 3.7754632\tbest: 3.7754632 (0)\ttotal: 532ms\tremaining: 2h 57m 20s\n",
      "100:\tlearn: 3.5846547\ttest: 3.6158900\tbest: 3.6158900 (100)\ttotal: 15.2s\tremaining: 49m 59s\n",
      "200:\tlearn: 3.5441143\ttest: 3.6133491\tbest: 3.6131050 (174)\ttotal: 28.2s\tremaining: 46m 21s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.612024692\n",
      "bestIteration = 241\n",
      "\n",
      "Shrink model to first 242 iterations.\n",
      "[-1.66221884 -0.23042346 -0.42724805 ...  0.43441855 -1.68430123\n",
      "  0.0744106 ]\n",
      "Fold 5 started at Mon Mar 21 22:23:25 2022\n",
      "0:\tlearn: 3.8305313\ttest: 3.9020982\tbest: 3.9020982 (0)\ttotal: 558ms\tremaining: 3h 6m 3s\n",
      "100:\tlearn: 3.5771977\ttest: 3.7168513\tbest: 3.7168513 (100)\ttotal: 15.1s\tremaining: 49m 42s\n",
      "200:\tlearn: 3.5293880\ttest: 3.7111231\tbest: 3.7109848 (199)\ttotal: 28.7s\tremaining: 47m 6s\n",
      "300:\tlearn: 3.5019767\ttest: 3.7084618\tbest: 3.7084618 (300)\ttotal: 41.3s\tremaining: 45m 1s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.707570543\n",
      "bestIteration = 321\n",
      "\n",
      "Shrink model to first 322 iterations.\n",
      "[-1.982061   -0.28584165 -0.50753396 ...  0.50661972 -2.07059879\n",
      "  0.09813628]\n",
      "Fold 6 started at Mon Mar 21 22:25:01 2022\n",
      "0:\tlearn: 3.8337296\ttest: 3.8622683\tbest: 3.8622683 (0)\ttotal: 631ms\tremaining: 3h 30m 29s\n",
      "100:\tlearn: 3.5840018\ttest: 3.6891156\tbest: 3.6890835 (99)\ttotal: 14.7s\tremaining: 48m 17s\n",
      "200:\tlearn: 3.5407707\ttest: 3.6829645\tbest: 3.6829590 (199)\ttotal: 27.9s\tremaining: 45m 52s\n",
      "300:\tlearn: 3.5081291\ttest: 3.6817114\tbest: 3.6815802 (288)\ttotal: 41.7s\tremaining: 45m 31s\n",
      "400:\tlearn: 3.4764866\ttest: 3.6813162\tbest: 3.6813162 (400)\ttotal: 55.2s\tremaining: 44m 57s\n",
      "500:\tlearn: 3.4450371\ttest: 3.6810094\tbest: 3.6802456 (453)\ttotal: 1m 7s\tremaining: 43m 55s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.680245612\n",
      "bestIteration = 453\n",
      "\n",
      "Shrink model to first 454 iterations.\n",
      "[-2.33249202 -0.32300096 -0.60848687 ...  0.58778593 -2.36378594\n",
      "  0.10915682]\n",
      "Fold 7 started at Mon Mar 21 22:26:54 2022\n",
      "0:\tlearn: 3.8432191\ttest: 3.7810547\tbest: 3.7810547 (0)\ttotal: 503ms\tremaining: 2h 47m 46s\n",
      "100:\tlearn: 3.5871022\ttest: 3.5801305\tbest: 3.5801305 (100)\ttotal: 14.9s\tremaining: 48m 47s\n",
      "200:\tlearn: 3.5376994\ttest: 3.5689070\tbest: 3.5687409 (199)\ttotal: 28s\tremaining: 46m\n",
      "300:\tlearn: 3.5049472\ttest: 3.5648358\tbest: 3.5646574 (283)\ttotal: 40.7s\tremaining: 44m 23s\n",
      "400:\tlearn: 3.4680395\ttest: 3.5618090\tbest: 3.5615917 (395)\ttotal: 53.3s\tremaining: 43m 27s\n",
      "500:\tlearn: 3.4320939\ttest: 3.5594137\tbest: 3.5594034 (496)\ttotal: 1m 6s\tremaining: 42m 55s\n",
      "600:\tlearn: 3.4006740\ttest: 3.5589551\tbest: 3.5585435 (564)\ttotal: 1m 20s\tremaining: 43m 9s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.558543547\n",
      "bestIteration = 564\n",
      "\n",
      "Shrink model to first 565 iterations.\n",
      "[-2.60452285 -0.35988197 -0.72648073 ...  0.67136126 -2.70109236\n",
      "  0.12935196]\n",
      "Fold 8 started at Mon Mar 21 22:42:43 2022\n",
      "0:\tlearn: 3.8462754\ttest: 3.7505656\tbest: 3.7505656 (0)\ttotal: 390ms\tremaining: 2h 9m 57s\n",
      "100:\tlearn: 3.5893770\ttest: 3.5773491\tbest: 3.5773491 (100)\ttotal: 16s\tremaining: 52m 28s\n",
      "200:\tlearn: 3.5420873\ttest: 3.5702746\tbest: 3.5702746 (200)\ttotal: 30.6s\tremaining: 50m 11s\n",
      "300:\tlearn: 3.5164643\ttest: 3.5679658\tbest: 3.5678097 (294)\ttotal: 43.5s\tremaining: 47m 28s\n",
      "400:\tlearn: 3.4915315\ttest: 3.5662991\tbest: 3.5661312 (380)\ttotal: 56.7s\tremaining: 46m 12s\n",
      "500:\tlearn: 3.4631843\ttest: 3.5638687\tbest: 3.5636031 (492)\ttotal: 1m 9s\tremaining: 45m 10s\n",
      "600:\tlearn: 3.4320343\ttest: 3.5620941\tbest: 3.5620941 (600)\ttotal: 1m 22s\tremaining: 44m 30s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.560234271\n",
      "bestIteration = 639\n",
      "\n",
      "Shrink model to first 640 iterations.\n",
      "[-3.00271205 -0.39184834 -0.81135842 ...  0.75442914 -3.0727943\n",
      "  0.14100635]\n",
      "Fold 9 started at Mon Mar 21 22:51:12 2022\n",
      "0:\tlearn: 3.8233904\ttest: 3.9598842\tbest: 3.9598842 (0)\ttotal: 587ms\tremaining: 3h 15m 35s\n",
      "100:\tlearn: 3.5717427\ttest: 3.7932100\tbest: 3.7932100 (100)\ttotal: 15.1s\tremaining: 49m 25s\n",
      "200:\tlearn: 3.5288985\ttest: 3.7865194\tbest: 3.7860235 (192)\ttotal: 28.2s\tremaining: 46m 21s\n",
      "300:\tlearn: 3.4963267\ttest: 3.7845620\tbest: 3.7843167 (280)\ttotal: 41.4s\tremaining: 45m 7s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.784316741\n",
      "bestIteration = 280\n",
      "\n",
      "Shrink model to first 281 iterations.\n",
      "[-3.35219798 -0.4283507  -0.92136316 ...  0.83352659 -3.42502993\n",
      "  0.162562  ]\n",
      "CV mean score: 3.6462, std: 0.0923.\n",
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Mon Mar 21 22:52:43 2022\n",
      "0:\tlearn: 1.7074964\ttest: 1.6992073\tbest: 1.6992073 (0)\ttotal: 420ms\tremaining: 2h 19m 50s\n",
      "100:\tlearn: 1.5445835\ttest: 1.5552532\tbest: 1.5552532 (100)\ttotal: 15.3s\tremaining: 50m 13s\n",
      "200:\tlearn: 1.5252847\ttest: 1.5499425\tbest: 1.5499425 (200)\ttotal: 29.6s\tremaining: 48m 35s\n",
      "300:\tlearn: 1.5086291\ttest: 1.5484731\tbest: 1.5484058 (287)\ttotal: 43s\tremaining: 46m 55s\n",
      "400:\tlearn: 1.4944008\ttest: 1.5477609\tbest: 1.5477107 (397)\ttotal: 56.2s\tremaining: 45m 47s\n",
      "500:\tlearn: 1.4796665\ttest: 1.5473827\tbest: 1.5473827 (500)\ttotal: 1m 9s\tremaining: 45m 3s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.547217152\n",
      "bestIteration = 530\n",
      "\n",
      "Shrink model to first 531 iterations.\n",
      "[-0.04015235 -0.03195681 -0.03559937 ...  0.09092392 -0.08766023\n",
      "  0.00925281]\n",
      "Fold 1 started at Mon Mar 21 22:54:50 2022\n",
      "0:\tlearn: 1.7042363\ttest: 1.7296382\tbest: 1.7296382 (0)\ttotal: 531ms\tremaining: 2h 56m 56s\n",
      "100:\tlearn: 1.5419424\ttest: 1.5770844\tbest: 1.5770844 (100)\ttotal: 15.5s\tremaining: 50m 44s\n",
      "200:\tlearn: 1.5224746\ttest: 1.5710298\tbest: 1.5710057 (199)\ttotal: 30.6s\tremaining: 50m 12s\n",
      "300:\tlearn: 1.5063435\ttest: 1.5688605\tbest: 1.5688605 (300)\ttotal: 52.6s\tremaining: 57m 36s\n",
      "400:\tlearn: 1.4930475\ttest: 1.5676946\tbest: 1.5676902 (398)\ttotal: 1m 7s\tremaining: 55m 7s\n",
      "500:\tlearn: 1.4800630\ttest: 1.5671589\tbest: 1.5671589 (500)\ttotal: 1m 21s\tremaining: 52m 57s\n",
      "600:\tlearn: 1.4673515\ttest: 1.5669341\tbest: 1.5668330 (596)\ttotal: 1m 35s\tremaining: 51m 14s\n",
      "700:\tlearn: 1.4545747\ttest: 1.5662621\tbest: 1.5662602 (699)\ttotal: 1m 49s\tremaining: 50m 13s\n",
      "800:\tlearn: 1.4418049\ttest: 1.5660748\tbest: 1.5659808 (767)\ttotal: 2m 3s\tremaining: 49m 28s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.565980771\n",
      "bestIteration = 767\n",
      "\n",
      "Shrink model to first 768 iterations.\n",
      "[-0.07246528 -0.05271176 -0.1032884  ...  0.20129174 -0.15891286\n",
      "  0.02605243]\n",
      "Fold 2 started at Mon Mar 21 23:00:10 2022\n",
      "0:\tlearn: 1.7082259\ttest: 1.6922539\tbest: 1.6922539 (0)\ttotal: 397ms\tremaining: 2h 12m 26s\n",
      "100:\tlearn: 1.5450028\ttest: 1.5521840\tbest: 1.5521840 (100)\ttotal: 15.1s\tremaining: 49m 28s\n",
      "200:\tlearn: 1.5249552\ttest: 1.5470530\tbest: 1.5470530 (200)\ttotal: 29s\tremaining: 47m 32s\n",
      "300:\tlearn: 1.5085859\ttest: 1.5456808\tbest: 1.5456808 (300)\ttotal: 43.2s\tremaining: 47m 10s\n",
      "400:\tlearn: 1.4936152\ttest: 1.5451938\tbest: 1.5450674 (357)\ttotal: 58s\tremaining: 47m 13s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.545067364\n",
      "bestIteration = 357\n",
      "\n",
      "Shrink model to first 358 iterations.\n",
      "[-0.11881331 -0.08030689 -0.15042837 ...  0.28420399 -0.2438892\n",
      "  0.04348507]\n",
      "Fold 3 started at Mon Mar 21 23:01:55 2022\n",
      "0:\tlearn: 1.7081799\ttest: 1.6935528\tbest: 1.6935528 (0)\ttotal: 663ms\tremaining: 3h 40m 50s\n",
      "100:\tlearn: 1.5453831\ttest: 1.5466188\tbest: 1.5466188 (100)\ttotal: 16s\tremaining: 52m 32s\n",
      "200:\tlearn: 1.5253560\ttest: 1.5409122\tbest: 1.5409122 (200)\ttotal: 31.2s\tremaining: 51m 10s\n",
      "300:\tlearn: 1.5094102\ttest: 1.5391384\tbest: 1.5391384 (300)\ttotal: 45s\tremaining: 49m 4s\n",
      "400:\tlearn: 1.4954955\ttest: 1.5386468\tbest: 1.5386217 (397)\ttotal: 58.5s\tremaining: 47m 39s\n",
      "500:\tlearn: 1.4819108\ttest: 1.5381136\tbest: 1.5381128 (475)\ttotal: 1m 12s\tremaining: 46m 45s\n",
      "600:\tlearn: 1.4686974\ttest: 1.5377007\tbest: 1.5376499 (575)\ttotal: 1m 25s\tremaining: 46m 5s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.537606248\n",
      "bestIteration = 612\n",
      "\n",
      "Shrink model to first 613 iterations.\n",
      "[-0.16069881 -0.09586522 -0.20408269 ...  0.38336704 -0.32634175\n",
      "  0.07089124]\n",
      "Fold 4 started at Mon Mar 21 23:04:14 2022\n",
      "0:\tlearn: 1.7098268\ttest: 1.6786446\tbest: 1.6786446 (0)\ttotal: 617ms\tremaining: 3h 25m 30s\n",
      "100:\tlearn: 1.5462590\ttest: 1.5382564\tbest: 1.5382564 (100)\ttotal: 15.6s\tremaining: 51m 19s\n",
      "200:\tlearn: 1.5262969\ttest: 1.5331464\tbest: 1.5331254 (199)\ttotal: 34.1s\tremaining: 56m 3s\n",
      "300:\tlearn: 1.5106063\ttest: 1.5315657\tbest: 1.5315331 (294)\ttotal: 49.8s\tremaining: 54m 16s\n",
      "400:\tlearn: 1.4962361\ttest: 1.5313458\tbest: 1.5313443 (379)\ttotal: 1m 4s\tremaining: 52m 11s\n",
      "500:\tlearn: 1.4828927\ttest: 1.5312978\tbest: 1.5311262 (480)\ttotal: 1m 18s\tremaining: 50m 56s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.531126215\n",
      "bestIteration = 480\n",
      "\n",
      "Shrink model to first 481 iterations.\n",
      "[-0.19741778 -0.14701968 -0.25881478 ...  0.47148351 -0.40679213\n",
      "  0.08460767]\n",
      "Fold 5 started at Mon Mar 21 23:06:20 2022\n",
      "0:\tlearn: 1.7080914\ttest: 1.6951689\tbest: 1.6951689 (0)\ttotal: 531ms\tremaining: 2h 56m 55s\n",
      "100:\tlearn: 1.5452647\ttest: 1.5474677\tbest: 1.5474677 (100)\ttotal: 15.8s\tremaining: 51m 54s\n",
      "200:\tlearn: 1.5247677\ttest: 1.5416236\tbest: 1.5416236 (200)\ttotal: 30s\tremaining: 49m 11s\n",
      "300:\tlearn: 1.5092053\ttest: 1.5396373\tbest: 1.5396355 (299)\ttotal: 44.2s\tremaining: 48m 12s\n",
      "400:\tlearn: 1.4949119\ttest: 1.5383012\tbest: 1.5383012 (400)\ttotal: 58.5s\tremaining: 47m 39s\n",
      "500:\tlearn: 1.4812903\ttest: 1.5377764\tbest: 1.5377764 (500)\ttotal: 1m 12s\tremaining: 47m 15s\n",
      "600:\tlearn: 1.4684148\ttest: 1.5375557\tbest: 1.5375557 (600)\ttotal: 1m 26s\tremaining: 46m 43s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.537512784\n",
      "bestIteration = 601\n",
      "\n",
      "Shrink model to first 602 iterations.\n",
      "[-0.23050883 -0.16697063 -0.3211679  ...  0.55676571 -0.4811262\n",
      "  0.09972359]\n",
      "Fold 6 started at Mon Mar 21 23:08:38 2022\n",
      "0:\tlearn: 1.7035333\ttest: 1.7363041\tbest: 1.7363041 (0)\ttotal: 488ms\tremaining: 2h 42m 46s\n",
      "100:\tlearn: 1.5424848\ttest: 1.5711289\tbest: 1.5711289 (100)\ttotal: 16.2s\tremaining: 53m 8s\n",
      "200:\tlearn: 1.5229374\ttest: 1.5630933\tbest: 1.5630933 (200)\ttotal: 31.1s\tremaining: 50m 59s\n",
      "300:\tlearn: 1.5078843\ttest: 1.5602425\tbest: 1.5602027 (293)\ttotal: 45.6s\tremaining: 49m 46s\n",
      "400:\tlearn: 1.4917149\ttest: 1.5590959\tbest: 1.5590191 (396)\ttotal: 1m\tremaining: 48m 53s\n",
      "500:\tlearn: 1.4782840\ttest: 1.5586220\tbest: 1.5585948 (493)\ttotal: 1m 13s\tremaining: 47m 54s\n",
      "600:\tlearn: 1.4653000\ttest: 1.5581751\tbest: 1.5580342 (583)\ttotal: 1m 27s\tremaining: 46m 57s\n",
      "700:\tlearn: 1.4518939\ttest: 1.5578533\tbest: 1.5578533 (700)\ttotal: 1m 41s\tremaining: 46m 23s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.557787203\n",
      "bestIteration = 711\n",
      "\n",
      "Shrink model to first 712 iterations.\n",
      "[-0.27611473 -0.20800075 -0.36867251 ...  0.65805719 -0.54670092\n",
      "  0.1272273 ]\n",
      "Fold 7 started at Mon Mar 21 23:11:12 2022\n",
      "0:\tlearn: 1.7050530\ttest: 1.7224202\tbest: 1.7224202 (0)\ttotal: 615ms\tremaining: 3h 25m 3s\n",
      "100:\tlearn: 1.5427908\ttest: 1.5711838\tbest: 1.5711838 (100)\ttotal: 15.4s\tremaining: 50m 43s\n",
      "200:\tlearn: 1.5229967\ttest: 1.5648191\tbest: 1.5648191 (200)\ttotal: 29.6s\tremaining: 48m 31s\n",
      "300:\tlearn: 1.5074168\ttest: 1.5620910\tbest: 1.5620826 (299)\ttotal: 43.4s\tremaining: 47m 18s\n",
      "400:\tlearn: 1.4926963\ttest: 1.5608931\tbest: 1.5608778 (396)\ttotal: 56.8s\tremaining: 46m 17s\n",
      "500:\tlearn: 1.4783254\ttest: 1.5600500\tbest: 1.5599649 (494)\ttotal: 1m 10s\tremaining: 45m 42s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.559863536\n",
      "bestIteration = 528\n",
      "\n",
      "Shrink model to first 529 iterations.\n",
      "[-0.30857436 -0.22766093 -0.4169471  ...  0.75403715 -0.61638434\n",
      "  0.14837555]\n",
      "Fold 8 started at Mon Mar 21 23:13:17 2022\n",
      "0:\tlearn: 1.7071964\ttest: 1.7027340\tbest: 1.7027340 (0)\ttotal: 542ms\tremaining: 3h 43s\n",
      "100:\tlearn: 1.5435866\ttest: 1.5625038\tbest: 1.5625038 (100)\ttotal: 16s\tremaining: 52m 29s\n",
      "200:\tlearn: 1.5238371\ttest: 1.5566202\tbest: 1.5566202 (200)\ttotal: 30.4s\tremaining: 49m 56s\n",
      "300:\tlearn: 1.5084370\ttest: 1.5540297\tbest: 1.5539590 (298)\ttotal: 44.6s\tremaining: 48m 38s\n",
      "400:\tlearn: 1.4934966\ttest: 1.5527797\tbest: 1.5527797 (400)\ttotal: 58.1s\tremaining: 47m 20s\n",
      "500:\tlearn: 1.4800345\ttest: 1.5522829\tbest: 1.5522077 (466)\ttotal: 1m 11s\tremaining: 46m 25s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.552163094\n",
      "bestIteration = 511\n",
      "\n",
      "Shrink model to first 512 iterations.\n",
      "[-0.34770491 -0.2607853  -0.48003059 ...  0.84797788 -0.69130526\n",
      "  0.16169886]\n",
      "Fold 9 started at Mon Mar 21 23:15:21 2022\n",
      "0:\tlearn: 1.7054228\ttest: 1.7181334\tbest: 1.7181334 (0)\ttotal: 512ms\tremaining: 2h 50m 42s\n",
      "100:\tlearn: 1.5429423\ttest: 1.5705997\tbest: 1.5705997 (100)\ttotal: 15.3s\tremaining: 50m 23s\n",
      "200:\tlearn: 1.5227327\ttest: 1.5653258\tbest: 1.5653258 (200)\ttotal: 29.5s\tremaining: 48m 26s\n",
      "300:\tlearn: 1.5064686\ttest: 1.5635165\tbest: 1.5634981 (299)\ttotal: 43.3s\tremaining: 47m 14s\n",
      "400:\tlearn: 1.4916703\ttest: 1.5625876\tbest: 1.5625876 (400)\ttotal: 56.8s\tremaining: 46m 17s\n",
      "500:\tlearn: 1.4779761\ttest: 1.5617716\tbest: 1.5617037 (495)\ttotal: 1m 10s\tremaining: 45m 31s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.561703667\n",
      "bestIteration = 495\n",
      "\n",
      "Shrink model to first 496 iterations.\n",
      "[-0.38099971 -0.29762903 -0.52721955 ...  0.92699156 -0.77679689\n",
      "  0.18560785]\n",
      "CV mean score: 1.5496, std: 0.0112.\n",
      "========== 分类模型 ==========\n",
      "Fold 0 started at Mon Mar 21 23:17:21 2022\n",
      "0:\tlearn: 0.5887535\ttest: 0.5888299\tbest: 0.5888299 (0)\ttotal: 476ms\tremaining: 2h 38m 31s\n",
      "100:\tlearn: 0.0410958\ttest: 0.0452466\tbest: 0.0452466 (100)\ttotal: 17s\tremaining: 55m 58s\n",
      "200:\tlearn: 0.0388177\ttest: 0.0449700\tbest: 0.0449689 (199)\ttotal: 32.1s\tremaining: 52m 44s\n",
      "300:\tlearn: 0.0372326\ttest: 0.0447256\tbest: 0.0447246 (290)\ttotal: 47.1s\tremaining: 51m 19s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04470980359\n",
      "bestIteration = 328\n",
      "\n",
      "Shrink model to first 329 iterations.\n",
      "[0.00754971 0.0002185  0.00050332 ... 0.00076948 0.00764681 0.00056384]\n",
      "Fold 1 started at Mon Mar 21 23:19:07 2022\n",
      "0:\tlearn: 0.5890037\ttest: 0.5889655\tbest: 0.5889655 (0)\ttotal: 507ms\tremaining: 2h 49m 3s\n",
      "100:\tlearn: 0.0415723\ttest: 0.0429453\tbest: 0.0429453 (100)\ttotal: 16.4s\tremaining: 53m 48s\n",
      "200:\tlearn: 0.0392666\ttest: 0.0425277\tbest: 0.0425277 (200)\ttotal: 31.2s\tremaining: 51m 14s\n",
      "300:\tlearn: 0.0378667\ttest: 0.0424306\tbest: 0.0424132 (293)\ttotal: 45.3s\tremaining: 49m 26s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04241320968\n",
      "bestIteration = 293\n",
      "\n",
      "Shrink model to first 294 iterations.\n",
      "[0.01683016 0.00048234 0.00100768 ... 0.00161286 0.01461703 0.00108848]\n",
      "Fold 2 started at Mon Mar 21 23:20:43 2022\n",
      "0:\tlearn: 0.5886277\ttest: 0.5889796\tbest: 0.5889796 (0)\ttotal: 485ms\tremaining: 2h 41m 31s\n",
      "100:\tlearn: 0.0407201\ttest: 0.0492724\tbest: 0.0492724 (100)\ttotal: 16.6s\tremaining: 54m 31s\n",
      "200:\tlearn: 0.0383019\ttest: 0.0488147\tbest: 0.0488073 (192)\ttotal: 31.5s\tremaining: 51m 45s\n",
      "300:\tlearn: 0.0369252\ttest: 0.0486480\tbest: 0.0486452 (297)\ttotal: 45.4s\tremaining: 49m 33s\n",
      "400:\tlearn: 0.0355352\ttest: 0.0486210\tbest: 0.0485646 (377)\ttotal: 1m\tremaining: 48m 57s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04856455801\n",
      "bestIteration = 377\n",
      "\n",
      "Shrink model to first 378 iterations.\n",
      "[0.02503024 0.00068069 0.00161974 ... 0.00230458 0.02408778 0.00160237]\n",
      "Fold 3 started at Mon Mar 21 23:22:33 2022\n",
      "0:\tlearn: 0.5890912\ttest: 0.5888137\tbest: 0.5888137 (0)\ttotal: 456ms\tremaining: 2h 31m 58s\n",
      "100:\tlearn: 0.0416527\ttest: 0.0418553\tbest: 0.0418546 (98)\ttotal: 16.8s\tremaining: 55m 13s\n",
      "200:\tlearn: 0.0394518\ttest: 0.0415739\tbest: 0.0415272 (169)\ttotal: 31.5s\tremaining: 51m 46s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04152723768\n",
      "bestIteration = 169\n",
      "\n",
      "Shrink model to first 170 iterations.\n",
      "[0.03330653 0.00090615 0.00216241 ... 0.00312971 0.03246174 0.00229554]\n",
      "Fold 4 started at Mon Mar 21 23:23:52 2022\n",
      "0:\tlearn: 0.5888999\ttest: 0.5888538\tbest: 0.5888538 (0)\ttotal: 516ms\tremaining: 2h 51m 52s\n",
      "100:\tlearn: 0.0413716\ttest: 0.0436096\tbest: 0.0436096 (100)\ttotal: 17.2s\tremaining: 56m 31s\n",
      "200:\tlearn: 0.0392973\ttest: 0.0431683\tbest: 0.0431683 (200)\ttotal: 32s\tremaining: 52m 31s\n",
      "300:\tlearn: 0.0375086\ttest: 0.0430218\tbest: 0.0430014 (295)\ttotal: 46.7s\tremaining: 50m 59s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04300141995\n",
      "bestIteration = 295\n",
      "\n",
      "Shrink model to first 296 iterations.\n",
      "[0.04207344 0.00119627 0.00294686 ... 0.00385863 0.03890023 0.00291957]\n",
      "Fold 5 started at Mon Mar 21 23:25:32 2022\n",
      "0:\tlearn: 0.5887943\ttest: 0.5888948\tbest: 0.5888948 (0)\ttotal: 539ms\tremaining: 2h 59m 48s\n",
      "100:\tlearn: 0.0413386\ttest: 0.0463430\tbest: 0.0463430 (100)\ttotal: 17s\tremaining: 55m 50s\n",
      "200:\tlearn: 0.0390779\ttest: 0.0458456\tbest: 0.0458456 (200)\ttotal: 31.9s\tremaining: 52m 19s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04581348295\n",
      "bestIteration = 208\n",
      "\n",
      "Shrink model to first 209 iterations.\n",
      "[0.05143377 0.00133632 0.00340744 ... 0.00471186 0.04368708 0.00360467]\n",
      "Fold 6 started at Mon Mar 21 23:26:56 2022\n",
      "0:\tlearn: 0.5888138\ttest: 0.5888656\tbest: 0.5888656 (0)\ttotal: 494ms\tremaining: 2h 44m 30s\n",
      "100:\tlearn: 0.0415207\ttest: 0.0453532\tbest: 0.0453532 (100)\ttotal: 16.3s\tremaining: 53m 39s\n",
      "200:\tlearn: 0.0387390\ttest: 0.0448212\tbest: 0.0448013 (195)\ttotal: 31.2s\tremaining: 51m 16s\n",
      "300:\tlearn: 0.0373003\ttest: 0.0446858\tbest: 0.0446790 (298)\ttotal: 45.9s\tremaining: 50m 4s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.0446789659\n",
      "bestIteration = 298\n",
      "\n",
      "Shrink model to first 299 iterations.\n",
      "[0.06011366 0.0015517  0.00396672 ... 0.0055976  0.05044558 0.00417578]\n",
      "Fold 7 started at Mon Mar 21 23:28:36 2022\n",
      "0:\tlearn: 0.5880870\ttest: 0.5880454\tbest: 0.5880454 (0)\ttotal: 389ms\tremaining: 2h 9m 32s\n",
      "100:\tlearn: 0.0412808\ttest: 0.0424259\tbest: 0.0424259 (100)\ttotal: 17s\tremaining: 55m 42s\n",
      "200:\tlearn: 0.0387405\ttest: 0.0420867\tbest: 0.0420867 (200)\ttotal: 31.9s\tremaining: 52m 23s\n",
      "300:\tlearn: 0.0368195\ttest: 0.0419899\tbest: 0.0419778 (296)\ttotal: 46.9s\tremaining: 51m 11s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04197782833\n",
      "bestIteration = 296\n",
      "\n",
      "Shrink model to first 297 iterations.\n",
      "[0.06953611 0.00173973 0.00519724 ... 0.00638793 0.05844432 0.00480953]\n",
      "Fold 8 started at Mon Mar 21 23:30:16 2022\n",
      "0:\tlearn: 0.5872860\ttest: 0.5870692\tbest: 0.5870692 (0)\ttotal: 557ms\tremaining: 3h 5m 32s\n",
      "100:\tlearn: 0.0414121\ttest: 0.0426416\tbest: 0.0426416 (100)\ttotal: 16.3s\tremaining: 53m 40s\n",
      "200:\tlearn: 0.0393739\ttest: 0.0422804\tbest: 0.0422804 (200)\ttotal: 30.5s\tremaining: 50m 7s\n",
      "300:\tlearn: 0.0379436\ttest: 0.0421528\tbest: 0.0421459 (297)\ttotal: 44.5s\tremaining: 48m 29s\n",
      "400:\tlearn: 0.0361801\ttest: 0.0420283\tbest: 0.0420216 (395)\ttotal: 58.8s\tremaining: 47m 52s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04200884569\n",
      "bestIteration = 410\n",
      "\n",
      "Shrink model to first 411 iterations.\n",
      "[0.07497489 0.00194433 0.00565921 ... 0.00704312 0.06721846 0.0052649 ]\n",
      "Fold 9 started at Mon Mar 21 23:32:09 2022\n",
      "0:\tlearn: 0.5865751\ttest: 0.5865582\tbest: 0.5865582 (0)\ttotal: 452ms\tremaining: 2h 30m 38s\n",
      "100:\tlearn: 0.0413746\ttest: 0.0481779\tbest: 0.0481731 (97)\ttotal: 16.5s\tremaining: 54m 7s\n",
      "200:\tlearn: 0.0386717\ttest: 0.0475376\tbest: 0.0475265 (199)\ttotal: 33.1s\tremaining: 54m 22s\n",
      "300:\tlearn: 0.0372936\ttest: 0.0474099\tbest: 0.0474067 (297)\ttotal: 47.3s\tremaining: 51m 32s\n",
      "400:\tlearn: 0.0359423\ttest: 0.0474313\tbest: 0.0473580 (359)\ttotal: 1m\tremaining: 49m 40s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04735801092\n",
      "bestIteration = 359\n",
      "\n",
      "Shrink model to first 360 iterations.\n",
      "[0.08232044 0.00217288 0.00618444 ... 0.0080909  0.07259306 0.00581075]\n",
      "CV mean score: 0.0442, std: 0.0023.\n"
     ]
    }
   ],
   "source": [
    "#### cat\n",
    "cat_params = {'learning_rate': 0.05, 'depth': 9, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "              'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False}\n",
    "folds = KFold(n_splits=10, shuffle=True, random_state=18)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_cat , predictions_cat , scores_cat  = train_model(X_train , X_test, y_train , params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_ncat, predictions_ncat, scores_ncat = train_model(X_ntrain, X_test, y_ntrain, params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "oof_bcat, predictions_bcat, scores_bcat = train_model(X_train , X_test, y_train_binary, params=cat_params, folds=folds, model_type='cat', eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../../data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_cat\n",
    "sub_df.to_csv('../../predictions_cat.csv', index=False)\n",
    "\n",
    "oof_cat  = pd.DataFrame(oof_cat)\n",
    "oof_ncat = pd.DataFrame(oof_ncat)\n",
    "oof_bcat = pd.DataFrame(oof_bcat)\n",
    "\n",
    "predictions_cat  = pd.DataFrame(predictions_cat)\n",
    "predictions_ncat = pd.DataFrame(predictions_ncat)\n",
    "predictions_bcat = pd.DataFrame(predictions_bcat)\n",
    "\n",
    "oof_cat.to_csv('../../result/oof_cat.csv',header=None,index=False)\n",
    "oof_bcat.to_csv('../../result/oof_bcat.csv',header=None,index=False)\n",
    "oof_ncat.to_csv('../../result/oof_ncat.csv',header=None,index=False)\n",
    "\n",
    "predictions_cat.to_csv('../../result/predictions_cat.csv',header=None,index=False)\n",
    "predictions_ncat.to_csv('../../result/predictions_ncat.csv',header=None,index=False)\n",
    "predictions_bcat.to_csv('../../result/predictions_bcat.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### lgb\n",
    "# oof_lgb  = pd.read_csv('./result/oof_lgb.csv',header=None)\n",
    "# oof_nlgb = pd.read_csv('./result/oof_nlgb.csv',header=None)\n",
    "# oof_blgb = pd.read_csv('./result/oof_blgb.csv',header=None)\n",
    "\n",
    "# predictions_lgb  = pd.read_csv('./result/predictions_lgb.csv',header=None)\n",
    "# predictions_nlgb = pd.read_csv('./result/predictions_nlgb.csv',header=None)\n",
    "# predictions_blgb = pd.read_csv('./result/predictions_blgb.csv',header=None)\n",
    "\n",
    "# #### xgb\n",
    "# oof_xgb  = pd.read_csv('./result/oof_xgb.csv',header=None)\n",
    "# oof_nxgb = pd.read_csv('./result/oof_nxgb.csv',header=None)\n",
    "# oof_bxgb = pd.read_csv('./result/oof_bxgb.csv',header=None)\n",
    "\n",
    "# predictions_xgb  = pd.read_csv('./result/predictions_xgb.csv',header=None)\n",
    "# predictions_nxgb = pd.read_csv('./result/predictions_nxgb.csv',header=None)\n",
    "# predictions_bxgb = pd.read_csv('./result/predictions_bxgb.csv',header=None)\n",
    "\n",
    "# #### cat\n",
    "# oof_cat  = pd.read_csv('./result/oof_cat.csv',header=None)\n",
    "# oof_ncat = pd.read_csv('./result/oof_ncat.csv',header=None)\n",
    "# oof_bcat = pd.read_csv('./result/oof_bcat.csv',header=None)\n",
    "\n",
    "# predictions_cat  = pd.read_csv('./result/predictions_cat.csv',header=None)\n",
    "# predictions_ncat = pd.read_csv('./result/predictions_ncat.csv',header=None)\n",
    "# predictions_bcat = pd.read_csv('./result/predictions_bcat.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../../data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_lgb + predictions_xgb + predictions_cat) / 3\n",
    "sub_df.to_csv('../../predictions_wei_average.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 0----------\n",
      "fold n°2\n",
      "----------Stacking 1----------\n",
      "fold n°3\n",
      "----------Stacking 2----------\n",
      "fold n°4\n",
      "----------Stacking 3----------\n",
      "fold n°5\n",
      "----------Stacking 4----------\n",
      "fold n°6\n",
      "----------Stacking 5----------\n",
      "fold n°7\n",
      "----------Stacking 6----------\n",
      "fold n°8\n",
      "----------Stacking 7----------\n",
      "fold n°9\n",
      "----------Stacking 8----------\n",
      "fold n°10\n",
      "----------Stacking 9----------\n",
      "mean:  3.639551215295588\n",
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 0----------\n",
      "fold n°2\n",
      "----------Stacking 1----------\n",
      "fold n°3\n",
      "----------Stacking 2----------\n",
      "fold n°4\n",
      "----------Stacking 3----------\n",
      "fold n°5\n",
      "----------Stacking 4----------\n",
      "fold n°6\n",
      "----------Stacking 5----------\n",
      "fold n°7\n",
      "----------Stacking 6----------\n",
      "fold n°8\n",
      "----------Stacking 7----------\n",
      "fold n°9\n",
      "----------Stacking 8----------\n",
      "fold n°10\n",
      "----------Stacking 9----------\n",
      "mean:  1.548240201689886\n",
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 0----------\n",
      "fold n°2\n",
      "----------Stacking 1----------\n",
      "fold n°3\n",
      "----------Stacking 2----------\n",
      "fold n°4\n",
      "----------Stacking 3----------\n",
      "fold n°5\n",
      "----------Stacking 4----------\n",
      "fold n°6\n",
      "----------Stacking 5----------\n",
      "fold n°7\n",
      "----------Stacking 6----------\n",
      "fold n°8\n",
      "----------Stacking 7----------\n",
      "fold n°9\n",
      "----------Stacking 8----------\n",
      "fold n°10\n",
      "----------Stacking 9----------\n",
      "mean:  0.04387677589507385\n"
     ]
    }
   ],
   "source": [
    "#### stack 回归模型  without-outliers回归模型 分类模型\n",
    "def stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3, y, eval_type='regression'):\n",
    "   \n",
    "    train_stack = np.vstack([oof_1, oof_2]).transpose()\n",
    "    test_stack = np.vstack([predictions_1, predictions_2]).transpose()\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2020)\n",
    "    oof = np.zeros(train_stack.shape[0])\n",
    "    predictions = np.zeros(test_stack.shape[0])\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, y)):\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        trn_data, trn_y = train_stack[trn_idx], y[trn_idx]\n",
    "        val_data, val_y = train_stack[val_idx], y[val_idx]\n",
    "        print(\"-\" * 10 + \"Stacking \" + str(fold_) + \"-\" * 10)\n",
    "        clf = BayesianRidge()\n",
    "        clf.fit(trn_data, trn_y)\n",
    "\n",
    "        oof[val_idx] = clf.predict(val_data)\n",
    "        predictions += clf.predict(test_stack) / (5 * 2)\n",
    "    if eval_type == 'regression':\n",
    "        print('mean: ',np.sqrt(mean_squared_error(y, oof)))\n",
    "    if eval_type == 'binary':\n",
    "        print('mean: ',log_loss(y, oof))\n",
    "    \n",
    "    return oof, predictions\n",
    "print('='*30)\n",
    "oof_stack , predictions_stack  = stack_model(oof_lgb[0] , oof_xgb[0] , oof_cat[0] , predictions_lgb[0] , predictions_xgb[0] , predictions_cat[0] , target)\n",
    "print('='*30)\n",
    "oof_nstack, predictions_nstack = stack_model(oof_nlgb[0], oof_nxgb[0], oof_ncat[0], predictions_nlgb[0], predictions_nxgb[0], predictions_ncat[0], ntarget)\n",
    "print('='*30)\n",
    "oof_bstack, predictions_bstack = stack_model(oof_blgb[0], oof_bxgb[0], oof_bcat[0], predictions_blgb[0], predictions_bxgb[0], predictions_bcat[0], target_binary, eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../../data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_stack\n",
    "sub_df.to_csv('../../predictions_stack.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trick融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../../data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack\n",
    "sub_df.to_csv('../../predictions_trick.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../../data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack)*0.5 + predictions_stack*0.5\n",
    "sub_df.to_csv('../../predictions_trick&stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
